{
  "cells": [
    {
      "metadata": {
        "id": "1c58786e7d26ad61"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "In this assignment, you will investigate the precision issues in computing the gradient. You will also implement a simple linear regression model using the custom autograd engine."
      ],
      "id": "1c58786e7d26ad61"
    },
    {
      "metadata": {
        "id": "abb3e4038597e341"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1: Precision issues"
      ],
      "id": "abb3e4038597e341"
    },
    {
      "metadata": {
        "jupyter": {
          "is_executing": true
        },
        "id": "9d637ecc07f917b6",
        "outputId": "c846cf45-36e0-4eea-ddb3-1544156661d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x + 5\n",
        "\n",
        "def gradient(f, x, h=0.0001):\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "gradient(f, 2)"
      ],
      "id": "9d637ecc07f917b6",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.000300000023941"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "initial_id",
        "outputId": "07663f7c-1c51-4ef6-e12d-38b9cf5204ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# What happens if we keep decreasing h?\n",
        "gradient(f, 2, h=0.0000000000000001)\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Why is the gradient 0?                                                       #\n",
        "# If you don't know, google it!                                                #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# ANSWER:\n",
        "\n",
        "# The gradient is 0 because of floating-point precision errors.\n",
        "# When h becomes extremely small, the difference (f(x + h) - f(x)) may be too tiny to be accurately represented, causing it to round to zero.\n",
        "# This leads to the computed gradient being 0 instead of the actual derivative.\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "id": "initial_id",
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "execution_count": 4
    },
    {
      "metadata": {
        "id": "7f38cac6fc0e388d"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 2: Linear Regression\n",
        "\n",
        "Let's review the training loop of a simple linear regression model."
      ],
      "id": "7f38cac6fc0e388d"
    },
    {
      "metadata": {
        "id": "5bca901c841e73c5"
      },
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "id": "5bca901c841e73c5"
    },
    {
      "metadata": {
        "id": "3085115ded6fe281"
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "import random"
      ],
      "id": "3085115ded6fe281",
      "outputs": [],
      "execution_count": 5
    },
    {
      "metadata": {
        "id": "54917eb18f15a26a"
      },
      "cell_type": "code",
      "source": [
        "class Tensor:\n",
        "    def __init__(self, data, _children=(), _operation=''):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)\n",
        "        self.gradient = 0\n",
        "        self._backward = lambda: None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"tensor=({self.data})\"\n",
        "\n",
        "    def __add__(self, other):  # self + other\n",
        "        output = Tensor(self.data + other.data, (self, other), '+')\n",
        "        def _backward():\n",
        "            self.gradient = 1 * output.gradient\n",
        "            other.gradient = 1 * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def __mul__(self, other):  # self * other\n",
        "        output = Tensor(self.data * other.data, (self, other), '*')\n",
        "        def _backward():\n",
        "            self.gradient = other.data * output.gradient\n",
        "            other.gradient = self.data * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def tanh(self):  # tanh(self)\n",
        "        output = Tensor(math.tanh(self.data), (self,), 'tanh')\n",
        "        def _backward():\n",
        "            self.gradient = (1.0 - math.tanh(self.data) ** 2) * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def __pow__(self, power):  # self ** power\n",
        "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
        "        output = Tensor(self.data ** power, (self,), f'**{power}')\n",
        "        def _backward():\n",
        "            self.gradient = power * (self.data ** (power - 1)) * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * Tensor(-1.0)\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)"
      ],
      "id": "54917eb18f15a26a",
      "outputs": [],
      "execution_count": 6
    },
    {
      "metadata": {
        "id": "8c6a0bf8bbd6a560"
      },
      "cell_type": "markdown",
      "source": [
        "### Data\n",
        "\n",
        "**GOAL: Find the best line that fits the following data.**\n",
        "\n",
        "![Data](https://github.com/qorjiwon/LLM101n/blob/master/assets/linear.png?raw=1)\n",
        "\n",
        "(Image credit: MIT 18.06)\n",
        "\n",
        "(1, 1), (2, 2), (3, 2)"
      ],
      "id": "8c6a0bf8bbd6a560"
    },
    {
      "metadata": {
        "id": "76e76d698b2bd862"
      },
      "cell_type": "code",
      "source": [
        "# Input, Target data\n",
        "input = [Tensor(1), Tensor(2), Tensor(3)]\n",
        "target = [Tensor(1), Tensor(2), Tensor(2)]"
      ],
      "id": "76e76d698b2bd862",
      "outputs": [],
      "execution_count": 37
    },
    {
      "metadata": {
        "id": "26e07078367a6cf2"
      },
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "id": "26e07078367a6cf2"
    },
    {
      "metadata": {
        "id": "eb6d8bd7ee689ba5"
      },
      "cell_type": "code",
      "source": [
        "# Linear regression model\n",
        "class Linear:\n",
        "    def __init__(self):\n",
        "        self.a = Tensor(random.uniform(-1, 1))\n",
        "        self.b = Tensor(random.uniform(-1, 1))\n",
        "\n",
        "    def __call__(self, x):\n",
        "        y = self.a * x + self.b\n",
        "        return y\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.a, self.b"
      ],
      "id": "eb6d8bd7ee689ba5",
      "outputs": [],
      "execution_count": 95
    },
    {
      "metadata": {
        "id": "115e33323215ad51",
        "outputId": "89d451ff-a15d-4e9e-f33e-b0fa58c85bbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "model = Linear()\n",
        "\n",
        "# Example forward pass\n",
        "print(f\"Output: {model(input[0])}\")"
      ],
      "id": "115e33323215ad51",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output: tensor=(-0.14676618555264231)\n"
          ]
        }
      ],
      "execution_count": 109
    },
    {
      "metadata": {
        "id": "ef292ec8ddb33418"
      },
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "Implement the training loop for the linear regression model.\n",
        "\n",
        "Choose an appropriate learning rate."
      ],
      "id": "ef292ec8ddb33418"
    },
    {
      "metadata": {
        "id": "a593eadd7f471de7",
        "outputId": "34daee31-6b9f-4315-e64c-e38129f4184a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "lr = 2e-4  # learning rate\n",
        "\n",
        "# Training loop\n",
        "for step in range(10):\n",
        "    total_loss = Tensor(0)\n",
        "\n",
        "    # Forward pass\n",
        "    for x, y in zip(input, target):\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the forward pass.                                                  #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        loss = (model(x) - y) ** 2\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        total_loss += loss\n",
        "\n",
        "    # Backward pass\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    # Implement the backward pass.                                                 #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    total_loss.backward()\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # Update weights\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    # Update the weights of the model using the gradients.                         #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    for param in model.parameters():\n",
        "        param.data = param.data - lr * param.gradient\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    print(f\"Step: {step}, Loss: {total_loss.data}\")"
      ],
      "id": "a593eadd7f471de7",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0, Loss: 0.1752271399811644\n",
            "Step: 1, Loss: 0.17528633643702055\n",
            "Step: 2, Loss: 0.1751106680516117\n",
            "Step: 3, Loss: 0.17516957943355327\n",
            "Step: 4, Loss: 0.17522862834526534\n",
            "Step: 5, Loss: 0.17528781125719772\n",
            "Step: 6, Loss: 0.17534312449305675\n",
            "Step: 7, Loss: 0.17540257482020352\n",
            "Step: 8, Loss: 0.1754621488832028\n",
            "Step: 9, Loss: 0.1755218433005984\n"
          ]
        }
      ],
      "execution_count": 160
    },
    {
      "metadata": {
        "id": "9bf143c6ea52981f",
        "outputId": "dd7ad2b8-567d-42f4-b375-6870b3d00b17",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Print the final weights of the model\n",
        "print(f\"y = {model.a.data}*x + {model.b.data}\")"
      ],
      "id": "9bf143c6ea52981f",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "y = 0.4423618540228212*x + 0.7559280547894417\n"
          ]
        }
      ],
      "execution_count": 161
    },
    {
      "metadata": {
        "id": "f780baeabe64ada1"
      },
      "cell_type": "markdown",
      "source": [
        "## Extra Credit\n",
        "\n",
        "Linear regression is the simplest form of neural networks. It actually does not require gradient descent to solve for the weights.\n",
        "\n",
        "**Find a way to get the weights of the linear regression model without using gradient descent.**"
      ],
      "id": "f780baeabe64ada1"
    },
    {
      "metadata": {
        "id": "969d8988286f10fc",
        "outputId": "e19c9669-f2e2-4c09-e089-b64496722812",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# y = ax + b                                                                   #\n",
        "# x, y = (1, 1), (2, 2), (3, 2)                                                #\n",
        "# Find the values of a and b without using gradient descent.                   #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "# Given points\n",
        "x = [1, 2, 3]\n",
        "y = [1, 2, 2]\n",
        "\n",
        "# Calculate the sums\n",
        "sum_x = sum(x)\n",
        "sum_y = sum(y)\n",
        "sum_xy = sum(xi * yi for xi, yi in zip(x, y))\n",
        "sum_x_squared = sum(xi ** 2 for xi in x)\n",
        "N = len(x)\n",
        "\n",
        "# Calculate a and b using the normal equation\n",
        "a = (N * sum_xy - sum_x * sum_y) / (N * sum_x_squared - sum_x ** 2)\n",
        "b = (sum_y - a * sum_x) / N\n",
        "loss = 0\n",
        "for i in range(3):\n",
        "  loss += (a*x[i] - b - y[i])**2\n",
        "\n",
        "# Print the results\n",
        "print(f\"a = {a}\")\n",
        "print(f\"b = {b}\")\n",
        "print(f\"lost = {loss}\")\n",
        "\n",
        "# a = ??\n",
        "# b = ??\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
      ],
      "id": "969d8988286f10fc",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a = 0.5\n",
            "b = 0.6666666666666666\n",
            "lost = 5.499999999999998\n"
          ]
        }
      ],
      "execution_count": 162
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}