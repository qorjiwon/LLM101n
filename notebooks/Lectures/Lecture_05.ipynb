{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 5: Batch Normalization and Residual Streams\n",
    "\n",
    "In this lecture, we will discuss two important techniques that have been shown to be very effective in training deep neural networks: Batch Normalization and Residual Streams. We will discuss both of these techniques in detail and show how they can be used to improve the performance of deep neural networks."
   ],
   "id": "d7ec01368ced1292"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing libraries",
   "id": "33cf13e36926b778"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from utils import load_text, set_seed, configure_device"
   ],
   "id": "ec52a619440f8c07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "fe477370e7aeaa21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "\n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 16\n",
    "    d_hidden: int = 256\n",
    "\n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "\n",
    "    seed: int = 101\n",
    "\n",
    "config = MLPConfig()"
   ],
   "id": "a7081a3a12f4c51f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Reproducibility",
   "id": "1a5dcca49a43f084"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "set_seed(config.seed)",
   "id": "17f7c6b997aa28a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Device",
   "id": "3c7c489df3d723b7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "config.device = configure_device()",
   "id": "34090170590e322f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Dataset",
   "id": "963f2e27594e40f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load text and split by lines\n",
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ],
   "id": "211195ae5b5ff827",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tokenizer",
   "id": "922fb6664eebc180"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "da4a042a32e15b29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing",
   "id": "aab6f14cee8610db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train-Val Split\n",
    "train_names, val_names = train_test_split(names, test_size=config.val_size, random_state=config.seed)"
   ],
   "id": "eb227707a5b9fa21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class NamesDataset(Dataset):\n",
    "    def __init__(self, _names, context_size):\n",
    "        self.inputs, self.targets = [], []\n",
    "\n",
    "        for name in _names:\n",
    "            context = [0] * context_size\n",
    "\n",
    "            for char in name + \".\":\n",
    "                idx = str2idx[char]\n",
    "                self.inputs.append(context)\n",
    "                self.targets.append(idx)\n",
    "                context = context[1:] + [idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.inputs[idx])\n",
    "        target_id = torch.tensor(self.targets[idx])\n",
    "        return input_ids, target_id\n",
    "\n",
    "train_dataset = NamesDataset(train_names, config.context_size)\n",
    "val_dataset = NamesDataset(val_names, config.context_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)"
   ],
   "id": "6d16b7fb027f8777",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "82fe33a95a345eca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        # Hidden layer\n",
    "        h = F.tanh(x @ self.W1 + self.b1)  # (batch_size, d_hidden)\n",
    "\n",
    "        # Output layer\n",
    "        logits = h @ self.W2 + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "119716ca4c6043da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "mlp = MLP(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp.to(config.device) # Move the model to the device\n",
    "print(mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp.parameters()))"
   ],
   "id": "94f8f2319b138c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        max_steps: int,\n",
    "        lr: float,\n",
    "        val_interval: int,\n",
    "        device: torch.device\n",
    "):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_iter = itertools.cycle(train_loader)  # Infinite dataloader\n",
    "\n",
    "    for step in range(1, max_steps + 1):\n",
    "        model.train()\n",
    "        train_inputs, train_targets = next(train_iter)\n",
    "        train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(train_inputs)\n",
    "        loss = F.cross_entropy(logits, train_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        steps.append(step)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        if step % val_interval == 0:\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            total_samples = 0\n",
    "            with torch.no_grad():\n",
    "                for val_inputs, val_targets in val_loader:\n",
    "                    val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                    val_logits = model(val_inputs)\n",
    "                    batch_loss = F.cross_entropy(val_logits, val_targets)\n",
    "                    val_loss += batch_loss.item() * val_inputs.size(0)\n",
    "                    total_samples += val_inputs.size(0)\n",
    "            val_loss /= total_samples\n",
    "            val_losses.append(val_loss)\n",
    "            print(f\"Step {step}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if step == 1:\n",
    "            print(f\"Initial Train Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Plot the loss\n",
    "    plt.figure()\n",
    "    plt.plot(steps, train_losses, label=\"Train\")\n",
    "    val_steps = [step for step in steps if step % val_interval == 0]\n",
    "    plt.plot(val_steps, val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "acc7ca6e3536e47d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlp,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "93ae4dca234d06ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 1: Batch Normalization\n",
    "\n",
    "Recall what we do for initializing the weights of a neural network.\n",
    "\n",
    "1. We don't want the logits to be too big because the softmax might explode.\n",
    "- Initialize the final layer with small values.\n",
    "2. We don't want the gradients to vanish.\n",
    "- Initialize the inner layer with small values.\n",
    "- Use different activation functions.\n",
    "\n",
    "Eventually, what we want is to preserve the same gaussian distribution of the activations.\n",
    "\n",
    "**Why not just normalize the activations?** -> Key idea to [Batch Normalization](https://arxiv.org/pdf/1502.03167)"
   ],
   "id": "71e92363d3f47e3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MLP Model with Batch Normalization\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.gamma = nn.Parameter(torch.ones(1, d_hidden))\n",
    "        self.beta = nn.Parameter(torch.zeros(1, d_hidden))\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        # Hidden layer\n",
    "        x = x @ self.W1 + self.b1  # (batch_size, d_hidden)\n",
    "\n",
    "        # Batch Normalization\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the batch normalization for the hidden layer.                      #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        h = F.tanh(x)  # (batch_size, d_hidden)\n",
    "\n",
    "        # Output layer\n",
    "        logits = h @ self.W2 + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "b6945392c5082fa8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "mlp2 = MLP2(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp2.to(config.device) # Move the model to the device\n",
    "print(mlp2)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp2.parameters()))"
   ],
   "id": "d7abc4c3f15cfb3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlp2,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "5019152c82a20106",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's add more layers to the model.",
   "id": "f37dbe4519c2f0a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# MLP Model with more layers\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden, kaiming_init=False, batch_norm=False):\n",
    "        super().__init__()\n",
    "        # embedding layer\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        # hidden layers\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, d_hidden * 2))\n",
    "        self.W3 = nn.Parameter(torch.randn(d_hidden * 2, d_hidden * 4))\n",
    "        self.W4 = nn.Parameter(torch.randn(d_hidden * 4, d_hidden * 2))\n",
    "        self.W5 = nn.Parameter(torch.randn(d_hidden * 2, d_hidden))\n",
    "        # output layer\n",
    "        self.W6 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Where did the biases go?                                                     #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.kaiming_init = kaiming_init\n",
    "        if self.kaiming_init:\n",
    "            nn.init.kaiming_normal_(self.W1, mode='fan_in', nonlinearity='tanh')\n",
    "            nn.init.kaiming_normal_(self.W2, mode='fan_in', nonlinearity='tanh')\n",
    "            nn.init.kaiming_normal_(self.W3, mode='fan_in', nonlinearity='tanh')\n",
    "            nn.init.kaiming_normal_(self.W4, mode='fan_in', nonlinearity='tanh')\n",
    "            nn.init.kaiming_normal_(self.W5, mode='fan_in', nonlinearity='tanh')\n",
    "\n",
    "        # Batch Normalization\n",
    "        self.batch_norm = batch_norm\n",
    "        self.gamma1 = nn.Parameter(torch.ones(1, d_hidden))\n",
    "        self.beta1 = nn.Parameter(torch.zeros(1, d_hidden))\n",
    "        self.gamma2 = nn.Parameter(torch.ones(1, d_hidden * 2))\n",
    "        self.beta2 = nn.Parameter(torch.zeros(1, d_hidden * 2))\n",
    "        self.gamma3 = nn.Parameter(torch.ones(1, d_hidden * 4))\n",
    "        self.beta3 = nn.Parameter(torch.zeros(1, d_hidden * 4))\n",
    "        self.gamma4 = nn.Parameter(torch.ones(1, d_hidden * 2))\n",
    "        self.beta4  = nn.Parameter(torch.zeros(1, d_hidden* 2))\n",
    "        self.gamma5 = nn.Parameter(torch.ones(1, d_hidden))\n",
    "        self.beta5  = nn.Parameter(torch.zeros(1, d_hidden))\n",
    "\n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        # Hidden layer 1\n",
    "        x = x @ self.W1  # (batch_size, d_hidden)\n",
    "        if self.batch_norm:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std = x.std(dim=0, keepdim=True) + 1e-8\n",
    "            x = self.gamma1 * (x - mean) / std + self.beta1\n",
    "        h1 = F.tanh(x)\n",
    "\n",
    "        # Hidden layer 2\n",
    "        x = h1 @ self.W2\n",
    "        if self.batch_norm:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std = x.std(dim=0, keepdim=True) + 1e-8\n",
    "            x = self.gamma2 * (x - mean) / std + self.beta2\n",
    "        h2 = F.tanh(x)\n",
    "\n",
    "        # Hidden layer 3\n",
    "        x = h2 @ self.W3\n",
    "        if self.batch_norm:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std = x.std(dim=0, keepdim=True) + 1e-8\n",
    "            x = self.gamma3 * (x - mean) / std + self.beta3\n",
    "        h3 = F.tanh(x)\n",
    "\n",
    "        # Hidden layer 4\n",
    "        x = h3 @ self.W4  # (batch_size, d_hidden)\n",
    "        if self.batch_norm:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std  = x.std(dim=0, keepdim=True) + 1e-8\n",
    "            x = self.gamma4 * (x - mean) / std + self.beta4\n",
    "        h4 = torch.tanh(x)\n",
    "\n",
    "        # Hidden layer 5\n",
    "        x = h4 @ self.W5  # (batch_size, d_hidden)\n",
    "        if self.batch_norm:\n",
    "            mean = x.mean(dim=0, keepdim=True)\n",
    "            std  = x.std(dim=0, keepdim=True) + 1e-8\n",
    "            x = self.gamma5 * (x - mean) / std + self.beta5\n",
    "        h5 = torch.tanh(x)\n",
    "\n",
    "        # Output layer\n",
    "        logits = h5 @ self.W6  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "b688e421adff83f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# mlp3 = Kaiming_init=False, BatchNorm=False\n",
    "# mlp4 = Kaiming_init=True, BatchNorm=False\n",
    "# mlp5 = Kaiming_init=True, BatchNorm=True\n",
    "\n",
    "mlp3 = MLP3(config.vocab_size, config.context_size, config.d_embed, config.d_hidden, kaiming_init=False, batch_norm=False)\n",
    "mlp3.to(config.device)\n",
    "\n",
    "mlp4 = MLP3(config.vocab_size, config.context_size, config.d_embed, config.d_hidden, kaiming_init=True, batch_norm=False)\n",
    "mlp4.to(config.device)\n",
    "\n",
    "mlp5 = MLP3(config.vocab_size, config.context_size, config.d_embed, config.d_hidden, kaiming_init=True, batch_norm=True)\n",
    "mlp5.to(config.device)"
   ],
   "id": "63c93c44faa13cd8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlp3,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "3fd33a0f4621368",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlp4,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "b66d2134566b7e00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=mlp5,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "9e78551eca0ae952",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_gradients(model):\n",
    "    named_params = list(model.named_parameters())\n",
    "\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    legends = []\n",
    "\n",
    "    for name, p in named_params:\n",
    "        if p.requires_grad and p.grad is not None:\n",
    "            if p.ndim == 2 and not ('gamma' in name or 'beta' in name) and name in ['W1', 'W2', 'W3', 'W4', 'W5']:\n",
    "                t = p.grad.cpu()\n",
    "                weight_std = p.cpu().std() + 1e-8\n",
    "                print('weight %10s (%s) | mean %+f | std %e | gradient:weight ratio %e' %\n",
    "                      (tuple(p.shape), name, t.mean(), t.std(), t.std() / weight_std))\n",
    "\n",
    "                hy, hx = torch.histogram(t, bins=30, density=True)\n",
    "                plt.plot(hx[:-1].detach(), hy.detach())\n",
    "                legends.append(f'{name} {tuple(p.shape)}')\n",
    "\n",
    "    plt.legend(legends)\n",
    "    plt.title('Weights Gradient Distribution')\n",
    "    plt.xlabel('Gradient Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(20, 4))"
   ],
   "id": "e2a7e7372840bd8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_gradients(mlp3)",
   "id": "38daa684a28ac045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_gradients(mlp4)",
   "id": "d40a673bd26ecb0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_gradients(mlp5)",
   "id": "1d6c06dc16c4e459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "So basically what batch normalization is doing...\n",
    "- It normalizes the activations by subtracting the mean and dividing by the standard deviation.\n",
    "    - How do you get the mean and standard deviation?\n",
    "    - You calculate them for each feature over the batch.\n",
    "        - What about the test time? Batch size is 1.\n",
    "        - You calculate the entire mean and standard deviation over the training set. Use that for test time.\n",
    "\n"
   ],
   "id": "30afff95cf9af401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Batch Normalization for Test Time",
   "id": "b7b43d7979ebf952",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Part 2: Residual Streams\n",
    "\n",
    "One more idea to avoid vanishing gradients is to use **skip connections**.\n",
    "\n",
    "![Residual Streams](../../assets/residual.png)\n",
    "\n",
    "Recall what backpropagation does:\n",
    "- It sends the gradients to the weights of the previous layer over and over again.\n",
    "- As the number of layers increases, the gradients become smaller and smaller.\n",
    "- Eventually, the gradients become too small to update the weights.\n",
    "- The network stops learning.\n",
    "\n",
    "**Residual Streams** solve this problem by adding the input to the output.\n"
   ],
   "id": "fd11003168b561ef"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden, n_layers, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        self.layers.append(nn.Linear(context_size * d_embed, d_hidden))\n",
    "        self.batch_norms.append(nn.BatchNorm1d(d_hidden))\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.layers.append(nn.Linear(d_hidden, d_hidden))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(d_hidden))\n",
    "\n",
    "        self.out = nn.Linear(d_hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "\n",
    "        x = self.layers[0](x)\n",
    "        x = self.batch_norms[0](x)\n",
    "        x = F.tanh(x)\n",
    "        for layer, bn in zip(self.layers[1:], self.batch_norms[1:]):\n",
    "            residual = x  # Save the input for the skip connection.\n",
    "            x = layer(x)\n",
    "            x = bn(x)\n",
    "            x = F.tanh(x)\n",
    "            if self.residual:\n",
    "                x = x + residual  # Add the skip connection.\n",
    "\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ],
   "id": "a6e2d65e4cb16417",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "deep_mlp = DeepMLP(config.vocab_size, config.context_size, config.d_embed, config.d_hidden, n_layers=50, residual=False)\n",
    "deep_mlp.to(config.device) # Move the model to the device\n",
    "print(deep_mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in deep_mlp.parameters()))"
   ],
   "id": "76dca0c7427d3c75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "deep_mlp_res = DeepMLP(config.vocab_size, config.context_size, config.d_embed, config.d_hidden, n_layers=50, residual=True)\n",
    "deep_mlp_res.to(config.device) # Move the model to the device\n",
    "print(deep_mlp_res)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in deep_mlp_res.parameters()))"
   ],
   "id": "c42b5c9a94323596",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=deep_mlp,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "ac252ba0f15bb72c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train(\n",
    "    model=deep_mlp_res,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    max_steps=config.max_steps,\n",
    "    lr=config.lr,\n",
    "    val_interval=config.val_interval,\n",
    "    device=config.device\n",
    ")"
   ],
   "id": "cd67e81989d3d785",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_activations(model, x):\n",
    "    activations = {}\n",
    "    x_embed = model.C[x]\n",
    "    x_flat = x_embed.view(x.size(0), -1)\n",
    "\n",
    "    a = model.layers[0](x_flat)\n",
    "    a = model.batch_norms[0](a)\n",
    "    a = F.tanh(a)\n",
    "    activations[\"Layer 1\"] = a.detach().cpu().numpy()\n",
    "\n",
    "    for idx, (layer, bn) in enumerate(zip(model.layers[1:], model.batch_norms[1:]), start=2):\n",
    "        residual = a\n",
    "        a = layer(a)\n",
    "        a = bn(a)\n",
    "        a = F.tanh(a)\n",
    "        if model.residual:\n",
    "            a = a + residual\n",
    "        activations[f\"Layer {idx}\"] = a.detach().cpu().numpy()\n",
    "\n",
    "    return activations\n",
    "\n",
    "# Sample a batch from the train_loader.\n",
    "sample = next(iter(train_loader))[0]\n",
    "\n",
    "# Get activations for both non-residual and residual models.\n",
    "acts_no_res = get_activations(deep_mlp, sample)\n",
    "acts_res = get_activations(deep_mlp_res, sample)\n",
    "\n",
    "# Plot the activation histograms.\n",
    "import math\n",
    "cols = 5\n",
    "rows = math.ceil(len(acts_no_res) / cols)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 4 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, layer in enumerate(acts_no_res.keys()):\n",
    "    axes[idx].hist(acts_no_res[layer].flatten(), bins=50, alpha=0.5, label='No Residual')\n",
    "    axes[idx].hist(acts_res[layer].flatten(), bins=50, alpha=0.5, label='Residual')\n",
    "    axes[idx].set_title(layer)\n",
    "    axes[idx].set_xlabel(\"Activation Value\")\n",
    "    axes[idx].set_ylabel(\"Frequency\")\n",
    "    axes[idx].legend()\n",
    "\n",
    "# Turn off any extra subplots.\n",
    "for j in range(idx + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9ae8f18139c462f0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Conclusion\n",
    "\n",
    "Deep, deep neural networks is all about optimization.\n",
    "- [VGGNet, 2014](https://arxiv.org/pdf/1409.1556): 19 layers (\"very deep\" for 19 layers...)\n",
    "- [ResNet, 2015](https://arxiv.org/pdf/1512.03385): 152 layers\n",
    "- [GPT-3, 2020](https://arxiv.org/pdf/2005.14165): 96 layers\n",
    "\n",
    "Why didn't researchers add layers for better performance before?\n",
    "\n",
    "**Because it wasn't optimizable.**\n",
    "\n",
    "Batch Normalization and Residual Streams are two key techniques that made it possible to train deep neural networks such as **Transformers**.\n",
    "\n"
   ],
   "id": "6e9d0aaef87b4263"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Reproduce VGGNet, ResNet and compare the results.",
   "id": "a4e2aa02cbba2cc4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
