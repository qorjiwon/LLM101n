{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 4: Initialization and Activations\n",
    "\n",
    "In this lecture, we will discuss the importance of initialization and activation functions in neural networks."
   ],
   "id": "98e47033fa23b699"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Complex architectures\n",
    "\n",
    "In the previous lectures, we have seen how to build a single layer, and a multi-layer neural network. In practice, we often use more complex architectures, such as convolutional neural network (CNN), or a recurrent neural network (RNN).\n",
    "- MLP: [wikipedia](https://en.wikipedia.org/wiki/Multilayer_perceptron)\n",
    "- CNN: [LeCun et al. 1989](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)\n",
    "- RNN: [Sutskever et al. 2011](https://arxiv.org/pdf/1409.3215)\n",
    "\n",
    "In order to proceed to those more complex architectures, we need to dive deeper into **activation functions** and **gradients**."
   ],
   "id": "cd8105e0378300a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MLP\n",
    "\n",
    "Let's continue with our MLP model."
   ],
   "id": "1d2544107637bb44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing libraries",
   "id": "876ddcc8e04fb710"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from utils import load_text, set_seed, configure_device"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "aff861f3906e686"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "    device: torch.device = torch.device('cpu')  # Automatic device configuration\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "    \n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 16\n",
    "    d_hidden: int = 256\n",
    "    \n",
    "    # Training\n",
    "    val_size: float = 0.1\n",
    "    batch_size: int = 32\n",
    "    max_steps: int = 1000\n",
    "    lr: float = 0.01\n",
    "    val_interval: int = 100\n",
    "\n",
    "    seed: int = 101\n",
    "\n",
    "config = MLPConfig()"
   ],
   "id": "bc82cefceb2b4302",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reproducibility",
   "id": "e7e2cf68009ad20f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "set_seed(config.seed)\n",
    "generator = torch.Generator().manual_seed(config.seed)"
   ],
   "id": "53502de47cac985e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Device",
   "id": "a5c422a13101aa69"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "config.device = configure_device()",
   "id": "1d65cdb0ee95496a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "1830e0b05351c970"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load text and split by lines\n",
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ],
   "id": "dbe484251e8d3148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer",
   "id": "9eb1b7200ed36a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "fd282e2811924bf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing",
   "id": "3864f8e66657220e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Train-validation split",
   "id": "5113d265597ca2d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def split_names(_names, val_size):\n",
    "    if val_size <= 0 or val_size >= 1:\n",
    "        raise ValueError(f\"Invalid validation size: {val_size}\")\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Split the data into training and validation sets.                            #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return train_text, val_text"
   ],
   "id": "26528063b4665736",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train_names, val_names = split_names(names, config.val_size)",
   "id": "ddd8d1bd9057bdcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Dataloader",
   "id": "25ea5ab9af7b800d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_dataset(_names):\n",
    "    _inputs, _targets = [], []\n",
    "\n",
    "    for name in _names:\n",
    "        _context = [0] * config.context_size\n",
    "        \n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            _inputs.append(_context)\n",
    "            _targets.append(idx)\n",
    "            _context = _context[1:] + [idx]  # Shift the context by 1 character\n",
    "\n",
    "    _inputs = torch.tensor(_inputs)\n",
    "    _targets = torch.tensor(_targets)\n",
    "    \n",
    "    return _inputs, _targets"
   ],
   "id": "460c347fff0f8ba9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_inputs, train_targets = prepare_dataset(train_names)\n",
    "val_inputs, val_targets = prepare_dataset(val_names)"
   ],
   "id": "90a43d3870420b0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check the shapes\n",
    "print(f\"Training (Inputs, Targets) shapes: ({train_inputs.shape}, {train_targets.shape})\")\n",
    "print(f\"Validation (Inputs, Targets) shapes: ({val_inputs.shape}, {val_targets.shape})\")\n",
    "# Example input and target pairs\n",
    "print(f\"First Train (Input, target): ({train_inputs[0]}, {train_targets[0]})\")\n",
    "print(f\"First Validation (Input, Target): ({val_inputs[0]}, {val_targets[0]})\")\n",
    "print(f\"Second Train (Input, Target): ({train_inputs[1]}, {train_targets[1]})\")\n",
    "print(f\"Second Validation (Input, Target): ({val_inputs[1]}, {val_targets[1]})\")"
   ],
   "id": "6e83e4ee491ef185",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model",
   "id": "7a56951b7353fc88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size))\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size))\n",
    "        \n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "        \n",
    "        # Hidden layer\n",
    "        h = F.tanh(x @ self.W1 + self.b1)  # (batch_size, d_hidden)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = torch.matmul(h, self.W2) + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "8c63d0952da574d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the model\n",
    "mlp = MLP(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp.to(config.device) # Move the model to the device\n",
    "print(mlp)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp.parameters()))"
   ],
   "id": "fa2656e284cbc28a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Training",
   "id": "b1959eab47fe9149"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train(model):\n",
    "    steps = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config.lr)\n",
    "\n",
    "    for step in range(1, config.max_steps + 1):\n",
    "        # Training\n",
    "        model.train()  # Set the model to training mode\n",
    "        # Mini-batch\n",
    "        idx = torch.randperm(len(train_inputs))[:config.batch_size]\n",
    "        x, y = train_inputs[idx], train_targets[idx]\n",
    "        x, y = x.to(config.device), y.to(config.device)  # Move the data to the device\n",
    "\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the forward pass and the backward pass                             #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # Forward pass\n",
    "\n",
    "        # Backward pass\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        steps.append(step)\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        # Validation\n",
    "        if step % config.val_interval == 0:\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                val_logits = model(val_inputs.to(config.device))\n",
    "                val_loss = F.cross_entropy(val_logits, val_targets.to(config.device)).item()\n",
    "                val_losses.append(val_loss)\n",
    "            print(f\"Step {step}: Train Loss = {loss.item():.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        if step == 1:\n",
    "            print(f\"Initial Train Loss = {loss.item():.4f}\")\n",
    "\n",
    "    # Plot the loss\n",
    "    plt.figure()\n",
    "    plt.plot(steps, train_losses, label=\"Train\")\n",
    "    val_steps = [step for step in steps if step % config.val_interval == 0]\n",
    "    plt.plot(val_steps, val_losses, label=\"Validation\")\n",
    "    plt.xlabel(\"Steps\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "9464484496a413dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "train(mlp)",
   "id": "a98576264203d44b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Problem 1: Initialization\n",
    "\n",
    "Q: What is the initial loss value? What can you discuss about the loss value?"
   ],
   "id": "f150b3de2ddc6718"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Before the model is trained,                                                 #\n",
    "# we expect the model to predict a uniform distribution over the classes       #\n",
    "# What should be the expected loss value for a uniform distribution?           #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"Initial loss value: {expected_loss:.4f}\")"
   ],
   "id": "b8e6a354ade97cb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example of wrong initialization issue\n",
    "# Goal: Predict the correct class out of 4 classes\n",
    "#example_logits = torch.tensor([0, 0, 0, 0], dtype=torch.float32)  # Uniform distribution\n",
    "exemple_logits = torch.randn(4) * 1\n",
    "target = 0 # [1, 0, 0, 0]\n",
    "prob = F.softmax(exemple_logits, dim=-1)\n",
    "loss = -prob[target].log()\n",
    "\n",
    "print(f\"Example logits: {exemple_logits}\")\n",
    "print(f\"Example probabilities: {prob}\")\n",
    "print(f\"Example loss value: {loss:.4f}\")"
   ],
   "id": "ccc6f404e390df81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Solution: Re-initialize the model\n",
    "# Make the logits smaller\n",
    "class MLP2(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed))\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden))\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden))\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Re-initialize the model with a small value to get low logits                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "        \n",
    "        # Hidden layer\n",
    "        h = F.tanh(x @ self.W1 + self.b1)  # (batch_size, d_hidden)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = torch.matmul(h, self.W2) + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "6b98329c62af4a6d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the MLP V2\n",
    "mlp2 = MLP2(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp2.to(config.device) # Move the model to the device\n",
    "print(mlp2)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp2.parameters()))"
   ],
   "id": "8bca4ff3b2add3d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "# train(model=mlp2, max_steps=1)\n",
    "train(mlp2)"
   ],
   "id": "2fd54b99f41de6ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Initialization problem solved!\n",
    "\n",
    "Remember, initialization is a crucial step in training neural networks. If the weights are initialized with large values, the model may not converge. "
   ],
   "id": "65a9d790d7558226"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Problem 2: Activation functions\n",
    "\n",
    "Q: What is the activation function used in the hidden layer? What is the \n",
    "characteristic of this activation function?\n",
    "\n",
    "[PyTorch Tanh](https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html)"
   ],
   "id": "31b98f488b71a05b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "_idx = torch.randperm(len(train_inputs))[:config.batch_size]\n",
    "_x = train_inputs[_idx]\n",
    "_x_embed = mlp2.C[_x]\n",
    "_x = _x_embed.view(_x.size(0), -1)\n",
    "_x = _x @ mlp2.W1 + mlp2.b1\n",
    "h = F.tanh(_x)"
   ],
   "id": "a5f3313dc2f9b044",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the histogram of the hidden layer pre-activations and activations\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Pre-activations\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(_x.view(-1).tolist(), bins=50)\n",
    "plt.title(\"Hidden Layer Pre-Activations\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# Activations\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(h.view(-1).tolist(), bins=50, range=(-1, 1))\n",
    "plt.title(\"Hidden Layer Activations\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "id": "bfeccf6de3cc7ad7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# White if the value is larger than 0.99\n",
    "# Black if the value is smaller than 0.99\n",
    "plt.imshow(h.abs().cpu().detach().numpy() > 0.99, cmap=\"gray\", interpolation=\"nearest\")\n",
    "# If there is a column with all white, it means that the model will never learn -> Dead Neurons"
   ],
   "id": "571fbed636425c04",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Explain the problem with the graph above.                                    #\n",
    "# Hint: Lecture 1, custom tensor auto-grad engine.                             #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****"
   ],
   "id": "d0b56a2e0293c96b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Adding more layers and making the model deeper is known to have better generalization performance.\n",
    "\n",
    "The reason why people didn't add more layers in the past was because of the vanishing gradient problem."
   ],
   "id": "28c20fa2c5ba9de8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Solution1: Re-re-initialize the model\n",
    "# Make the pre-activations smaller\n",
    "class MLP3(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Re-initialize the model with a small value                                   #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "        \n",
    "        # Hidden layer\n",
    "        h = F.tanh(x @ self.W1 + self.b1)  # (batch_size, d_hidden)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = torch.matmul(h, self.W2) + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "ee9dc8dd22effe40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the MLP V3\n",
    "mlp3 = MLP3(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp3.to(config.device) # Move the model to the device\n",
    "print(mlp3)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp3.parameters()))"
   ],
   "id": "cb915aa001509161",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Forward pass\n",
    "_idx = torch.randperm(len(train_inputs))[:config.batch_size]\n",
    "_x = train_inputs[_idx]\n",
    "_x_embed = mlp3.C[_x]\n",
    "_x = _x_embed.view(_x.size(0), -1)\n",
    "_x = _x @ mlp3.W1 + mlp3.b1\n",
    "h = F.tanh(_x)"
   ],
   "id": "96d7e49148b3bb39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the histogram of the hidden layer pre-activations and activations\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Pre-activations\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(_x.view(-1).tolist(), bins=50)\n",
    "plt.title(\"Hidden Layer Pre-Activations\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# Activations\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(h.view(-1).tolist(), bins=50, range=(-1, 1))\n",
    "plt.title(\"Hidden Layer Activations\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "id": "d8aa4f82995db957",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "# White if the value is larger than 0.99\n",
    "# Black if the value is smaller than 0.99\n",
    "plt.imshow(h.abs().cpu().detach().numpy() > 0.99, cmap=\"gray\", interpolation=\"nearest\")\n",
    "# If there is a column with all white, it means that the model will never learn -> Dead Neurons"
   ],
   "id": "f97a9f00edafc114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "train(mlp3)"
   ],
   "id": "c4eda9dc0de2e7f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Summary**\n",
    "\n",
    "1. Raw model           -> Val Loss = 10.8837\n",
    "2. Initialization fix  -> Val Loss = 2.7808\n",
    "3. Saturation fix      -> Val Loss = 2.7467\n",
    "\n",
    "There was an improvement even in this simple model. Imagine the effect of these problems in a deeper model."
   ],
   "id": "e5b6af600b496ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Solution: Kaiming Initialization\n",
    "\n",
    "What should be the scale of the weights when initializing the model? (We can't try all the values every time)"
   ],
   "id": "b27d85e6dfce007f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example distribution\n",
    "x = torch.randn(1000, 10)  # 1000 samples, 10 features\n",
    "W = torch.randn(10, 200)   # 10 features, 200 hidden units\n",
    "y = x @ W\n",
    "\n",
    "# Plot the histogram of the output distribution\n",
    "plt.figure(figsize=(20, 5))\n",
    "# Input\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(x.view(-1).tolist(), bins=50, density=True, range=(-15, 15))\n",
    "plt.title(f\"mean = {x.mean().item():.4f}, std = {x.std().item():.4f}\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# Output\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y.view(-1).tolist(), bins=50, density=True, range=(-15, 15))\n",
    "plt.title(f\"mean = {y.mean().item():.4f}, std = {y.std().item():.4f}\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "id": "4ba19d83540d236f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# We want the same distribution for every layer (mean=0, std=1)                #\n",
    "# The gaussian distribution expanded from std=1 to std=3                       #\n",
    "# What should be the scale of the weights in the example above?                #\n",
    "################################################################################\n",
    "x = torch.randn(1000, 10)\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "W = torch.randn(10, 200) * scale\n",
    "y = x @ W\n",
    "\n",
    "# Plot the histogram of the output distribution\n",
    "plt.figure(figsize=(20, 5))\n",
    "# Input\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(x.view(-1).tolist(), bins=50, density=True)\n",
    "plt.title(f\"mean = {x.mean().item():.4f}, std = {x.std().item():.4f}\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# Output\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(y.view(-1).tolist(), bins=50, density=True)\n",
    "plt.title(f\"mean = {y.mean().item():.4f}, std = {y.std().item():.4f}\")\n",
    "plt.xlabel(\"Values\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ],
   "id": "dd7018abe53b9580",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Number of papers looked into this initialization problem: making the weights not shrink or explode.\n",
    "\n",
    "One of the most popular solutions is the Kaiming Initialization. [He et al. 2015](https://arxiv.org/pdf/1502.01852)\n",
    "\n",
    "PyTorch has a built-in function for Kaiming initialization. [PyTorch Documentation](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.kaiming_normal_)\n",
    "\n",
    "**scale = gain / sqrt(fan_in)**\n",
    "- gain\n",
    "\n",
    "![gain](../../assets/gain.png)\n",
    "\n",
    "- fan_in: number of input units"
   ],
   "id": "285f4a4e0dc9227e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Final solution: Re-re-re-initialize the model\n",
    "# Kaiming Initialization\n",
    "class MLP4(nn.Module):\n",
    "    def __init__(self, vocab_size, context_size, d_embed, d_hidden):\n",
    "        super().__init__()\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Re-initialize the model with Kaiming Initialization                           #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        self.C = nn.Parameter(torch.randn(vocab_size, d_embed) * self.embedding_scale)\n",
    "        self.W1 = nn.Parameter(torch.randn(context_size * d_embed, d_hidden) * self.hidden_scale)\n",
    "        self.b1 = nn.Parameter(torch.randn(d_hidden) * 0.1)\n",
    "        self.W2 = nn.Parameter(torch.randn(d_hidden, vocab_size) * self.output_scale)\n",
    "        self.b2 = nn.Parameter(torch.randn(vocab_size) * 0.01)\n",
    "        \n",
    "    def forward(self, x):  # x: (batch_size, context_size)\n",
    "        # Embedding\n",
    "        x_embed = self.C[x]  # (batch_size, context_size, d_embed)\n",
    "        x = x_embed.view(x.size(0), -1)  # (batch_size, context_size * d_embed)\n",
    "        \n",
    "        # Hidden layer\n",
    "        h = F.tanh(x @ self.W1 + self.b1)  # (batch_size, d_hidden)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = torch.matmul(h, self.W2) + self.b2  # (batch_size, vocab_size)\n",
    "        return logits"
   ],
   "id": "cdf7ea1f9900562d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize the MLP V4\n",
    "mlp4 = MLP4(config.vocab_size, config.context_size, config.d_embed, config.d_hidden)\n",
    "mlp4.to(config.device) # Move the model to the device\n",
    "print(mlp4)\n",
    "print(\"Number of parameters:\", sum(p.numel() for p in mlp4.parameters()))"
   ],
   "id": "596487c9aae3bd75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "train(mlp4)"
   ],
   "id": "63c34096b2dc3f0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Final val loss: 2.7497 Yay!",
   "id": "731277d6052c12"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
