{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ac1a934eac9271",
   "metadata": {},
   "source": [
    "# Lecture 1: Introduction to Machine Learning\n",
    "\n",
    "In this lecture, we will introduce the basics of machine learning.\n",
    "\n",
    "Let's start by exploring what neural network training looks like under the hood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b8f28897d2765a",
   "metadata": {},
   "source": [
    "## Part 1: Backpropagation\n",
    "\n",
    "Backpropagation algorithm is the cornerstone of training neural networks. (used in all neural nets) It is a method to calculate the gradient of the loss function with respect to the weights of the network.\n",
    "\n",
    "**The main problem is... how do computers compute the gradient?**\n",
    "\n",
    "Humans get the gradient by calculating the derivative, then plug in the values.\n",
    "\n",
    "Computers can't do that:\n",
    "1. Derivatives are hard to calculate\n",
    "2. Neural networks are huge\n",
    "\n",
    "Let's build our own backpropagation algorithm from scratch step by step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "afdaf122c0a24b14",
   "metadata": {},
   "source": [
    "# Importing libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f8b173bdbbdff33b",
   "metadata": {},
   "source": [
    "### Manual Gradient Calculation\n",
    "\n",
    "Let's manually calculate the gradient of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50519bf41896d5de",
   "metadata": {},
   "source": [
    "#### Example 1: Single Variable Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "79bd41eabcac06c0",
   "metadata": {},
   "source": [
    "# Define a random function\n",
    "def f(x):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # f(x) = 3x^2 - 4x + 5                                                         #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f29125addedc611",
   "metadata": {},
   "source": [
    "f(2)  # f(2) = 3*2^2 - 4*2 + 5 = 9"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34cf6c8c9460cedb",
   "metadata": {},
   "source": [
    "# Plot the function\n",
    "xs = np.arange(-5, 5, 0.25)  # np.arrange(start, stop, step)\n",
    "ys = [f(x) for x in xs]\n",
    "plt.plot(xs, ys)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8cc5601b2bfbdacf",
   "metadata": {},
   "source": [
    "What is the gradient of f(x) at x=2?\n",
    "- Human's solution:\n",
    "    - f'(x) = 6x - 4\n",
    "    - f'(2) = 6*2 - 4 = 8\n",
    "\n",
    "- Computer's solution:\n",
    "    - f'(2) = (f(2 + h) - f(2)) / h\n",
    "        - h: a small number (0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "id": "a4858fe8eb81441b",
   "metadata": {},
   "source": [
    "def gradient(function, x, _h=0.0001):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return grad"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6c0972b63c72fe7c",
   "metadata": {},
   "source": [
    "gradient(f, 2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c9d3eeb8dd980e7",
   "metadata": {},
   "source": [
    "Let's calculate the gradient of a more complex function.\n",
    "\n",
    "![(x + y) * z](../../assets/(x+y)z.png)\n",
    "\n",
    "(Image credit: Stanford CS231n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad426cfc324aeb",
   "metadata": {},
   "source": [
    "#### Example 2: Multi-Variable Function"
   ]
  },
  {
   "cell_type": "code",
   "id": "7c8986c366d8e99d",
   "metadata": {},
   "source": [
    "def f(x, y, z):\n",
    "    out = (x + y) * z\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "25857fe002e61146",
   "metadata": {},
   "source": [
    "f(-2, 5, -4)  # f(-2, 5, -4) = (-2 + 5) * -4 = -12"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a9171fd855b2b1b7",
   "metadata": {},
   "source": [
    "Let's calculate the gradient of this function\n",
    "\n",
    "**Goal**:\n",
    "1. df/dx at x=-2\n",
    "2. df/dy at y=5\n",
    "3. df/dz at z=-4"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2c53484de26a8ac",
   "metadata": {},
   "source": [
    "# Get the derivative of f(x, y, z) with respect to x, y, z\n",
    "h = 0.0001\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of f with respect to x=2                                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"df/dx: {df_dx}\")\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of f with respect to y=-3                                         #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"df/dy: {df_dy}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of f with respect to z=10                                         #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"df/dz: {df_dz}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4fecd506dd71a85",
   "metadata": {},
   "source": [
    "This time, let's calculate the gradient using the chain rule.\n",
    "\n",
    "\n",
    "![chain rule](../../assets/chain_rule.png)\n",
    "\n",
    "(Image credit: Stanford CS231n)\n",
    "\n",
    "**Chain Rule**:\n",
    "\n",
    "q = x + y\n",
    "\n",
    "f = q * z\n",
    "\n",
    "- df/dx = df/dq * dq/dx\n",
    "- df/dy = df/dq * dq/dy"
   ]
  },
  {
   "cell_type": "code",
   "id": "26df18dea033aadf",
   "metadata": {},
   "source": [
    "# Get the derivative of f(x, y, z) with respect to x, y using the chain rule\n",
    "\n",
    "# Redefine the function f(x, y, z) and q(x, y)\n",
    "def f(q, z):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return out\n",
    "\n",
    "def q(x, y):\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    return out"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cb02602405abfbf8",
   "metadata": {},
   "source": [
    "# q\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of f with respect to q=-2                                         #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"df/dq: {df_dq}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3bf19d2256e21a37",
   "metadata": {},
   "source": [
    "# x\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of q with respect to x=-2                                         #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"dq/dx: {dq_dx}\")\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of f with respect to x=-2                                         #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"df/dx: {df_dx}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "22816305e040df95",
   "metadata": {},
   "source": [
    "# y\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of q with respect to y=5                                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"dq/dy: {dq_dy}\")\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Derivative of f with respect to y=5                                          #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "print(f\"df/dy: {df_dy}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1dd5a4407648515",
   "metadata": {},
   "source": [
    "### Custom Auto-Grad Engine\n",
    "\n",
    "Neural networks are a series of functions that are composed together. Each function is a layer in the network. To get the gradient, we need to calculate the derivative of each function using the chain rule. \n",
    "\n",
    "**Let's make a custom tensor object that calculates and stores the gradient of the tensor.**\n",
    "1. Support basic operations: addition, multiplication, tanh, power\n",
    "2. Calculate and store the gradient of the tensor\n",
    "\n",
    "- Why not use numpy arrays?:\n",
    "    - We need to keep track of the operations and tensors that lead to this tensor in order to calculate the gradient.\n",
    "\n",
    "- Numerical vs Analytical\n",
    "    - Numerical differentiation: estimates the gradient using the finite difference approximation\n",
    "        -  f'(x) = (f(x + h) - f(x)) / h\n",
    "        - Slow\n",
    "        - Precision issues\n",
    "    - Analytical differentiation: derives the function symbolically using the chain rule\n",
    "        - Fast\n",
    "        - Precise\n",
    "        - Need to code the derivative of every operation"
   ]
  },
  {
   "cell_type": "code",
   "id": "b279b1065a769ef0",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "# a = Tensor(-2.0)\n",
    "# b = Tensor(5.0)\n",
    "# c = Tensor(-4.0)\n",
    "# f = (a + b) * c\n",
    "# f.backward()\n",
    "\n",
    "# print(f\"a: {a}\")               # a: tensor=(-2.0)\n",
    "# print(f\"output: {f}\")          # output: tensor=12.0\n",
    "# print(f\"df/da: {a.gradient}\")  # df/da: 1.0\n",
    "# print(f\"df/db: {b.gradient}\")  # df/db: 1.0\n",
    "# print(f\"df/dc: {c.gradient}\")  # df/dc: 3.0"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71703911d4435375",
   "metadata": {},
   "source": [
    "#### Tensor version 1\n",
    "\n",
    "A simple tensor object that supports addition and multiplication."
   ]
  },
  {
   "cell_type": "code",
   "id": "b961d923e295de1f",
   "metadata": {},
   "source": [
    "class TensorV1:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    # method to print the tensor\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    # method to add two tensors\n",
    "    def __add__(self, other):  # self + other\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the addition operation.                                            #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return output\n",
    "\n",
    "    # method to multiply two tensors\n",
    "    def __mul__(self, other):  # self * other\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the multiplication operation.                                      #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return output"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79bced7cd35204a8",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "a = TensorV1(-2.0)\n",
    "b = TensorV1(5.0)\n",
    "c = TensorV1(-4.0)\n",
    "f = (a + b) * c\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"output: {f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "37036ba643a2220c",
   "metadata": {},
   "source": [
    "#### Tensor version 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff003f5d07d0e5dd",
   "metadata": {},
   "source": [
    "class TensorV2:\n",
    "    def __init__(self, data, _children=(), _operation=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)  # _children: tensors that lead to this tensor (ex: 2 * 3 = 6, 2 and 3 are children of 6)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = TensorV2(self.data + other.data, (self, other))\n",
    "        def _backward():\n",
    "            ################################################################################\n",
    "            # TODO:                                                                        #\n",
    "            # Implement the backward pass for addition.                                    #\n",
    "            # hint: use the chain rule (df/dx = df/dq * dq/dx, q = x + y)                  #\n",
    "            # df/dq -> output.gradient                                                     #\n",
    "            ################################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = TensorV2(self.data * other.data, (self, other))\n",
    "        def _backward():\n",
    "            ################################################################################\n",
    "            # TODO:                                                                        #\n",
    "            # Implement the backward pass for multiplication.                              #\n",
    "            # hint: use the chain rule (df/dx = df/dq * dq/dx, q = x * y)                  #\n",
    "            # df/dq -> output.gradient                                                     #\n",
    "            ################################################################################\n",
    "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    # method to calculate the gradient\n",
    "    # Goes through the graph in reverse topological order and calculate the gradient\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "17b5a3f0347949",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "a = TensorV2(-2.0)\n",
    "b = TensorV2(5.0)\n",
    "c = TensorV2(-4.0)\n",
    "f = (a + b) * c\n",
    "f.backward()\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"output: {f}\")\n",
    "print(f\"df/da: {a.gradient}\")\n",
    "print(f\"df/db: {b.gradient}\")\n",
    "print(f\"df/dc: {c.gradient}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb4b152ef95d1cd2",
   "metadata": {},
   "source": [
    "#### Tensor final version"
   ]
  },
  {
   "cell_type": "code",
   "id": "65624f2a4df6d632",
   "metadata": {},
   "source": [
    "class Tensor:\n",
    "    def __init__(self, data, _children=(), _operation='', label=''):\n",
    "        self.data = data\n",
    "        self._prev = set(_children)\n",
    "        self.gradient = 0\n",
    "        self._backward = lambda: None\n",
    "        # for visualization\n",
    "        self._operation = _operation  # _operation: operation that lead to this tensor (ex: 2 * 3 = 6, * is the operation)\n",
    "        self.label = label  # label: name of the tensor\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"tensor=({self.data})\"\n",
    "\n",
    "    def __add__(self, other):  # self + other\n",
    "        output = Tensor(self.data + other.data, (self, other), '+')\n",
    "        def _backward():\n",
    "            self.gradient += 1 * output.gradient\n",
    "            other.gradient += 1 * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    def __mul__(self, other):  # self * other\n",
    "        output = Tensor(self.data * other.data, (self, other), '*')\n",
    "        def _backward():\n",
    "            self.gradient += other.data * output.gradient\n",
    "            other.gradient += self.data * output.gradient\n",
    "        output._backward = _backward\n",
    "        return output\n",
    "\n",
    "    # Activation function\n",
    "    def tanh(self):  # tanh(self)\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the tanh operation.                                                #\n",
    "        # hint: use math.tanh                                                          #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return output\n",
    "\n",
    "    def __pow__(self, power):  # self ** power\n",
    "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # Implement the power operation.                                               #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        return output\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "        build_topo(self)\n",
    "        self.gradient = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "\n",
    "    def __neg__(self): # -self\n",
    "        return self * Tensor(-1.0)\n",
    "\n",
    "    def __sub__(self, other): # self - other\n",
    "        return self + (-other)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fac571f40884fec5",
   "metadata": {},
   "source": [
    "# Example usage\n",
    "a = Tensor(-2.0, label='a')\n",
    "b = Tensor(5.0, label='b')\n",
    "c = Tensor(-4.0, label='c')\n",
    "\n",
    "ab = a + b; ab.label = 'a + b'\n",
    "f = ab * c; f.label = 'f'\n",
    "\n",
    "f.backward()\n",
    "\n",
    "print(f\"a: {a}\")\n",
    "print(f\"output: {f}\")\n",
    "print(f\"df/da: {a.gradient}\")\n",
    "print(f\"df/db: {b.gradient}\")\n",
    "print(f\"df/dc: {c.gradient}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dc29c5a9851a9ca2",
   "metadata": {},
   "source": [
    "def trace(root):\n",
    "    # builds a set of all nodes and edges in a graph\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "    from graphviz import Digraph\n",
    "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "    nodes, edges = trace(root)\n",
    "    for n in nodes:\n",
    "        uid = str(id(n))\n",
    "        # for any value in the graph, create a rectangular ('record') node for it\n",
    "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.gradient), shape='record')\n",
    "        if n._operation:\n",
    "            # if this value is a result of some operation, create an op node for it\n",
    "            dot.node(name = uid + n._operation, label=n._operation)\n",
    "            # and connect this node to it\n",
    "            dot.edge(uid + n._operation, uid)\n",
    "    for n1, n2 in edges:\n",
    "        # connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._operation)\n",
    "    return dot"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e903d8c90f571a",
   "metadata": {},
   "source": [
    "# Draw the graph\n",
    "draw_dot(f)\n",
    "\n",
    "# If there is a problem with displaying the graph, run the following command in the terminal (Linux only)\n",
    "# sudo apt-get install graphviz"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bcb13aa09ef0f4af",
   "metadata": {},
   "source": "Now let's move on to a simple neural network."
  },
  {
   "cell_type": "code",
   "id": "3f9ebf150e297445",
   "metadata": {},
   "source": [
    "# Input values\n",
    "x1 = Tensor(2.0, label='x1')\n",
    "x2 = Tensor(3.0, label='x2')\n",
    "\n",
    "# Weights\n",
    "w1 = Tensor(-3.0, label='w1')\n",
    "w2 = Tensor(1.0, label='w2')\n",
    "\n",
    "# bias\n",
    "b = Tensor(6.0, label='b')\n",
    "\n",
    "# y = tanh(Wx + b) = tanh(w1*x1 + w2*x2 + b)\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "y = x1w1x2w2 + b\n",
    "y = y.tanh(); y.label = 'y'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e543c53d8ec409e",
   "metadata": {},
   "source": [
    "# Calculate the gradient of y with respect to w1, w2, and b\n",
    "y.backward()\n",
    "print(f\"y: {y.data}\")\n",
    "print(f\"dy/dw1: {w1.gradient}\")\n",
    "print(f\"dy/dw2: {w2.gradient}\")\n",
    "print(f\"dy/db: {b.gradient}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3046c24e1a94240a",
   "metadata": {},
   "source": [
    "# Draw the graph\n",
    "draw_dot(y)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "34972521d62160a6",
   "metadata": {},
   "source": [
    "### PyTorch Tensor\n",
    "\n",
    "PyTorch is a popular deep learning library that provides a tensor object similar to the one we've implemented.\n",
    "\n",
    "Let's implement the same example using PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "id": "254b7f18bff1fc44",
   "metadata": {},
   "source": [
    "import torch\n",
    "# Implement PyTorch tensor\n",
    "x1 = torch.tensor(2.0, requires_grad=False)\n",
    "x2 = torch.tensor(3.0, requires_grad=False)\n",
    "w1 = torch.tensor(-3.0, requires_grad=True)\n",
    "w2 = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(6.0, requires_grad=True)\n",
    "y = torch.tanh(w1 * x1 + w2 * x2 + b)\n",
    "\n",
    "# Calculate the gradient of y with respect to w1, w2, and b\n",
    "print(f\"y: {y.data.item()}\")\n",
    "y.backward()\n",
    "print(f\"dy/dw1: {w1.grad.item()}\")\n",
    "print(f\"dy/dw2: {w2.grad.item()}\")\n",
    "print(f\"dy/db: {b.grad.item()}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c17e490aa4f1c213",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression\n",
    "\n",
    "Linear regression is a simple machine learning model that predicts the relationship between two variables.\n",
    "\n",
    "Let's implement a simple linear regression model using our custom tensor object. Then, we will train the model using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd0afff9eb1fa5",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e28c89d89c8d4e8",
   "metadata": {},
   "source": [
    "# Linear network\n",
    "class Linear:\n",
    "    def __init__(self, input_size):\n",
    "        # y = Wx + b\n",
    "        self.weights = [Tensor(random.uniform(-1, 1)) for _ in range(input_size)]  # (input_size)\n",
    "        self.bias = Tensor(random.uniform(-1, 1))  # (1)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        y = Wx + b\n",
    "        \"\"\"\n",
    "        x = sum((wi * xi for wi, xi in zip(self.weights, x)), self.bias)  # matrix multiplication by summing the products\n",
    "        # note: Dot product is not good for parallelization\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.weights + [self.bias]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b74c7bf299837031",
   "metadata": {},
   "source": [
    "# Input values\n",
    "x = [Tensor(-2.0), Tensor(5.0), Tensor(-4.0), Tensor(1.0)]\n",
    "\n",
    "# Target value\n",
    "y = Tensor(-5.0)\n",
    "\n",
    "# Initialize the linear network\n",
    "model = Linear(input_size=4)\n",
    "\n",
    "# Example forward pass\n",
    "print(f\"Output: {model(x)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f10fd413ef00d81e",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Training a neural network involves the following steps:\n",
    "1. Forward pass: Calculate the predicted value.\n",
    "2. Loss: Calculate the difference between the predicted value and the target value.\n",
    "3. Backward pass: Calculate the gradient of the loss with respect to the weights.\n",
    "4. Update weights: Update the weights using the gradients.\n",
    "\n",
    "Repeat the process until the loss is minimized.\n",
    "\n",
    "Let's implement a simple training loop for the linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "33eb7fb9ff8fbdcb",
   "metadata": {},
   "source": [
    "lr = 0.01  # Learning rate\n",
    "\n",
    "# Training loop\n",
    "for step in range(10):\n",
    "    # Forward pass\n",
    "    logits = model(x)\n",
    "    \n",
    "    # Loss\n",
    "    loss = (logits - y) ** 2  # MSE loss\n",
    "\n",
    "    # Backward pass\n",
    "    for param in model.parameters():\n",
    "        param.gradient = 0  # Zero the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights\n",
    "    for param in model.parameters():\n",
    "        param.data = param.data - lr * param.gradient\n",
    "\n",
    "    print(f\"Step {step+1}, Loss: {loss.data}\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
