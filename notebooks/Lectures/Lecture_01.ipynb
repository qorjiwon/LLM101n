{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32ac1a934eac9271",
      "metadata": {
        "id": "32ac1a934eac9271"
      },
      "source": [
        "# Lecture 1: Introduction to Machine Learning\n",
        "\n",
        "In this lecture, we will introduce the basics of machine learning.\n",
        "\n",
        "Let's start by exploring what neural network training looks like under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b8f28897d2765a",
      "metadata": {
        "id": "82b8f28897d2765a"
      },
      "source": [
        "## Part 1: Backpropagation\n",
        "\n",
        "Backpropagation algorithm is the cornerstone of training neural networks. (used in all neural nets) It is a method to calculate the gradient of the loss function with respect to the weights of the network.\n",
        "\n",
        "**The main problem is... how do computers compute the gradient?**\n",
        "\n",
        "Humans get the gradient by calculating the derivative, then plug in the values.\n",
        "\n",
        "Computers can't do that:\n",
        "1. Derivatives are hard to calculate\n",
        "2. Neural networks are huge\n",
        "\n",
        "Let's build our own backpropagation algorithm from scratch step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "afdaf122c0a24b14",
      "metadata": {
        "id": "afdaf122c0a24b14"
      },
      "source": [
        "# Importing libraries\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f8b173bdbbdff33b",
      "metadata": {
        "id": "f8b173bdbbdff33b"
      },
      "source": [
        "### Manual Gradient Calculation\n",
        "\n",
        "Let's manually calculate the gradient of a function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50519bf41896d5de",
      "metadata": {
        "id": "50519bf41896d5de"
      },
      "source": [
        "#### Example 1: Single Variable Function"
      ]
    },
    {
      "cell_type": "code",
      "id": "79bd41eabcac06c0",
      "metadata": {
        "id": "79bd41eabcac06c0"
      },
      "source": [
        "# Define a random function\n",
        "def f(x):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    # f(x) = 3x^2 - 4x + 5                                                         #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    out = 3*x*x - 4*x + 5\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "5f29125addedc611",
      "metadata": {
        "id": "5f29125addedc611",
        "outputId": "a2fef77b-9c64-4951-8d24-97e4b2794ff2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f(2)  # f(2) = 3*2^2 - 4*2 + 5 = 9"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "id": "34cf6c8c9460cedb",
      "metadata": {
        "id": "34cf6c8c9460cedb",
        "outputId": "cdaaa7bf-2c3d-4140-c6c8-c4ea1e3b9351",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        }
      },
      "source": [
        "# Plot the function\n",
        "xs = np.arange(-5, 5, 0.25)  # np.arrange(start, stop, step)\n",
        "ys = [f(x) for x in xs]\n",
        "plt.plot(xs, ys)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x78b00eb0ab10>]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAQt9JREFUeJzt3Xl4VOXh9vHvmZnsy4QA2UhCwhr2fRMX1BRUXFBEqbihBa1gRVwKbcX2pzVuVV83sLYqWhDFimhVLKJCkbAFQfY9EAhZIDDZyDYz7x/BtFFUlknOLPfnus6lnJlM7oxcmdvnPOd5DLfb7UZERETEi1jMDiAiIiLyfSooIiIi4nVUUERERMTrqKCIiIiI11FBEREREa+jgiIiIiJeRwVFREREvI4KioiIiHgdm9kBzoTL5SI/P5+oqCgMwzA7joiIiJwCt9tNWVkZSUlJWCw/PUbikwUlPz+flJQUs2OIiIjIGcjLyyM5Ofknn+OTBSUqKgqo/wGjo6NNTiMiIiKnorS0lJSUlIbP8Z/ikwXlu8s60dHRKigiIiI+5lSmZ2iSrIiIiHgdFRQRERHxOiooIiIi4nVUUERERMTrqKCIiIiI11FBEREREa+jgiIiIiJeRwVFREREvI4KioiIiHid0y4oy5Yt44orriApKQnDMPjggw8aPe52u5kxYwaJiYmEhYWRmZnJzp07Gz2npKSEcePGER0dTUxMDLfffjvl5eVn9YOIiIiI/zjtglJRUUGvXr146aWXTvr4k08+yfPPP8+sWbNYtWoVERERjBgxgqqqqobnjBs3js2bN7N48WL+9a9/sWzZMiZOnHjmP4WIiIj4FcPtdrvP+IsNgwULFjBq1CigfvQkKSmJ++67j/vvvx8Ah8NBfHw8b7zxBmPHjmXr1q107dqVNWvW0L9/fwAWLVrEZZddxoEDB0hKSvrZ71taWordbsfhcGgvHhERER9xOp/fHp2DsnfvXgoKCsjMzGw4Z7fbGTRoENnZ2QBkZ2cTExPTUE4AMjMzsVgsrFq16qSvW11dTWlpaaOjKWwrKOX3Czby0Yb8Jnl9EREROTUeLSgFBQUAxMfHNzofHx/f8FhBQQFxcXGNHrfZbMTGxjY85/uysrKw2+0NR0pKiidjN1iytYg5q/bzxorcJnl9EREROTU+cRfP9OnTcTgcDUdeXl6TfJ8x/ZOxWQxy9h1le0FZk3wPERER+XkeLSgJCQkAFBYWNjpfWFjY8FhCQgJFRUWNHq+rq6OkpKThOd8XEhJCdHR0o6MpxEWFktmlfvTn7dX7m+R7iIiIyM/zaEFJT08nISGBJUuWNJwrLS1l1apVDBkyBIAhQ4Zw7NgxcnJyGp7zxRdf4HK5GDRokCfjnJFfDkoF4P11B6iqdZqcRkREJDDZTvcLysvL2bVrV8Of9+7dy/r164mNjSU1NZUpU6bw6KOP0rFjR9LT03nooYdISkpquNOnS5cuXHLJJUyYMIFZs2ZRW1vL5MmTGTt27CndwdPUzuvQijYxYRw8dpxPNh7imr7JZkcSEREJOKc9grJ27Vr69OlDnz59AJg6dSp9+vRhxowZADz44IPcfffdTJw4kQEDBlBeXs6iRYsIDQ1teI05c+aQkZHBxRdfzGWXXca5557LX//6Vw/9SGfHYjH45cD6Sbi6zCMiImKOs1oHxSxNvQ5KUWkVQx7/AqfLzeJ7z6djfJTHv4eIiEigMW0dFH8RFx1KZpf6W6HfXt00dwyJiIjIj1NB+RG/HFg/WfafmiwrIiLS7FRQfsR5HVvTJiYMx/FaPt10yOw4IiIiAUUF5UdYLQZjB5yYLLtKl3lERESakwrKTxjTPwWrxWB1bgm7irSyrIiISHNRQfkJCfZQLsrQZFkREZHmpoLyM27QZFkREZFmp4LyM87v1JokeyjHKmv5bPPJd1sWERERz1JB+RlWi8H1A+pHUeau0sqyIiIizUEF5RRcNyAZiwGr9pawu7jc7DgiIiJ+TwXlFCTawxomy87T/jwiIiJNTgXlFH23sux7OQeortNkWRERkaakgnKKLujUmkR7KEcra/lsc6HZcURERPyaCsopslktXNf/u5VldZlHRESkKamgnIbrBqRgMSB7zxH2aLKsiIhIk1FBOQ1tYsIY1vnEZNk1WllWRESkqaignCZNlhUREWl6Kiin6cLOrYmPDqGkooZ/a7KsiIhIk1BBOU02q4Xrv5ssqzVRREREmoQKyhm4bkAKhgErdh9h7+EKs+OIiIj4HRWUM5DcIpxhnVoDMG+NRlFEREQ8TQXlDDVMll17gJo6l8lpRERE/IsKyhm6KCOOuKgQjlTUsGhzgdlxRERE/IoKyhmyWS2MPTGK8o/sfSanERER8S8qKGfhhoGpWC0Gq3NL2FZQanYcERERv6GCchYS7KGM6BYPwJsaRREREfEYFZSzdNPgNAA++OYgpVW15oYRERHxEyooZ2lwu1g6xUdSWePknzkHzI4jIiLiF1RQzpJhGNw0uC0Ab63ch9vtNjmRiIiI71NB8YCr+yYTGWJjT3EFX+86YnYcERERn6eC4gGRITau6dsGgDezc80NIyIi4gdUUDzku8s8n28t5OCx4yanERER8W0qKB7SMT6Kwe1icbnh7VXan0dERORsqKB40M1D0oD6DQSr65zmhhEREfFhKige9Iuu8cRHh3C4vIZFm7Q/j4iIyJlSQfGgIKuFGwbWz0XRyrIiIiJnTgXFw345MAWbxSBn31E25zvMjiMiIuKTVFA8LC46lEu6JwDwlkZRREREzogKShP4brLsB+sP4qjU/jwiIiKnSwWlCQxIa0FGQhRVtS7m5+SZHUdERMTnqKA0AcMwuGlI/WTZOav243Jpfx4REZHToYLSREb1bkNUiI29hytYvuuw2XFERER8igpKE4kIsTG6XzKgW45FREROlwpKE7rxxP48X2wr5MDRSpPTiIiI+A4VlCbUIS6SoR1a4nLXz0URERGRU6OC0sRuGpwGwDtr8qiq1f48IiIip0IFpYlldokjyR5KSUUNn2w8ZHYcERERn6CC0sRsVgs3DEoFNFlWRETkVKmgNIPrB6QSZDVYn3eMjQe0P4+IiMjPUUFpBq2jQrisRyIAb2bnmhtGRETEB6igNJObT6wsu3BDPkfKq01OIyIi4t1UUJpJ39QW9Eq2U1PnYq5uORYREflJKijNxDAMbjs3HYA3V+6jps5lciIRERHvpYLSjC7tnkh8dAjFZdV8vDHf7DgiIiJeSwWlGQXbLNw8JA2Avy/fi9utXY5FRERORgWlmd0wMJUQm4VNB0tZk3vU7DgiIiJeSQWlmbWICOaavvW7HL+2fK/JaURERLyTCooJbhuaBsC/txSQV6JdjkVERL5PBcUEHeOjOK9jK1xumL0i1+w4IiIiXsfjBcXpdPLQQw+Rnp5OWFgY7du355FHHmk0IdTtdjNjxgwSExMJCwsjMzOTnTt3ejqKV/vuluN31uRRXl1nchoRERHv4vGC8sQTTzBz5kxefPFFtm7dyhNPPMGTTz7JCy+80PCcJ598kueff55Zs2axatUqIiIiGDFiBFVVVZ6O47Uu6Nia9q0jKKuuY/7aPLPjiIiIeBWPF5QVK1Zw1VVXMXLkSNLS0rj22msZPnw4q1evBupHT5577jn+8Ic/cNVVV9GzZ0/efPNN8vPz+eCDDzwdx2tZLAbjh9aPoryxIhenS7cci4iIfMfjBeWcc85hyZIl7NixA4ANGzawfPlyLr30UgD27t1LQUEBmZmZDV9jt9sZNGgQ2dnZJ33N6upqSktLGx3+4Jq+bbCHBbHvSCVfbCsyO46IiIjX8HhBmTZtGmPHjiUjI4OgoCD69OnDlClTGDduHAAFBQUAxMfHN/q6+Pj4hse+LysrC7vd3nCkpKR4OrYpwoNt/HJgKqBbjkVERP6XxwvKu+++y5w5c5g7dy7r1q1j9uzZPP3008yePfuMX3P69Ok4HI6GIy/Pf+Zs3DykLVaLQfaeI2zJ94+RIRERkbPl8YLywAMPNIyi9OjRg5tuuol7772XrKwsABISEgAoLCxs9HWFhYUNj31fSEgI0dHRjQ5/kRQTxmU9EgF47WuNooiIiEATFJTKykoslsYva7Vacbnqd+9NT08nISGBJUuWNDxeWlrKqlWrGDJkiKfj+ITvFm77cH0+xWXV5oYRERHxAh4vKFdccQV//vOf+fjjj8nNzWXBggU888wzXH311QAYhsGUKVN49NFH+fDDD9m4cSM333wzSUlJjBo1ytNxfEKf1Bb0SY2hxulizqp9ZscRERExnc3TL/jCCy/w0EMPcdddd1FUVERSUhJ33HEHM2bMaHjOgw8+SEVFBRMnTuTYsWOce+65LFq0iNDQUE/H8Rm3DU3n7v3f8I+V+/j1sPaE2KxmRxIRETGN4f7fJV59RGlpKXa7HYfD4TfzUeqcLs5/8kvyHVU8PaYX1/ZLNjuSiIiIR53O57f24vESNquFm89JA+Dvy/fig71RRETEY1RQvMjYASmEBVnZeqiUlXtKzI4jIiJiGhUULxITHszofm0A3XIsIiKBTQXFy3y3P8/nWwvZd6TC5DQiIiLmUEHxMu1bR3Jh59a43fWbCIqIiAQiFRQvdNu59aMo89ceoKyq1uQ0IiIizU8FxQud26EVneIjKa+u4+3V+82OIyIi0uxUULyQYRj86tx2ALy2PJeaOpfJiURERJqXCoqXuqpPEnFRIRSUVvHhhnyz44iIiDQrFRQvFWKzNsxF+euy3bhcWrhNREQChwqKF7thUCqRITZ2FJbz1Y4is+OIiIg0GxUULxYdGsS4QakAzFq6x+Q0IiIizUcFxcuNH5pOkNVg9d4S1u0/anYcERGRZqGC4uUS7KGM6l2//P1fNYoiIiIBQgXFB9xxQf0tx59tKWBPcbnJaURERJqeCooP6BAXRWaXeNxuePU/2kRQRET8nwqKj7jzxCjKP9cdoKisyuQ0IiIiTUsFxUf0T4ulX9sW1NS5mK1NBEVExM+poPiQO86vH0V5K3sf5dV1JqcRERFpOiooPiSzSzztWkdQWlXHPG0iKCIifkwFxYdYLEbDKMrfl+/VJoIiIuK3VFB8zKg+bWgdFcIhRxUfaRNBERHxUyooPibEZuW2ofWbCL6ybDdutzYRFBER/6OC4oMabSK4vdjsOCIiIh6nguKD7GFB3NCwieBuk9OIiIh4ngqKjxo/NI0gq8GqvSV8o00ERUTEz6ig+KhEexhXfbeJ4DJtIigiIv5FBcWHTTxxy/GizQXsPVxhchoRERHPUUHxYZ3io7g4I+7EJoIaRREREf+hguLj7rigPQDv5RyguKza5DQiIiKeoYLi4waktaBPaow2ERQREb+iguLjDMPgjvPrR1HezM6ltKrW5EQiIiJnTwXFDwzvGk+HuEhKq+p4K3uf2XFERETOmgqKH7BYDCZdWD+K8vfle6msqTM5kYiIyNlRQfETV/RMom3LcEoqapi7ar/ZcURERM6KCoqfsFkt3DWsfhTllWV7qKp1mpxIRETkzKmg+JGr+yTTJiaM4rJq3l2bZ3YcERGRM6aC4keCbRbuvKB+ddlZX+2mps5lciIREZEzo4LiZ8b0TyEuKoR8RxXvrztgdhwREZEzooLiZ0KDrA179Lz81W7qnBpFERER36OC4oduGJRKy4hg9pdU8uGGfLPjiIiInDYVFD8UHmzj9vPSAXjxy104XW6TE4mIiJweFRQ/ddPgttjDgthTXMGnmw6ZHUdEROS0qKD4qajQIMYPTQPgxS924dIoioiI+BAVFD82/px0IkNsbCso4/OthWbHEREROWUqKH7MHh7EzUPaAvDCF7twuzWKIiIivkEFxc/dfm46YUFWNh508NWOYrPjiIiInBIVFD/XMjKEcYNSAXhhyU6NooiIiE9QQQkAE89vR7DNwrr9x8jefcTsOCIiIj9LBSUAxEWHMnZAClA/F0VERMTbqaAEiDsuaE+Q1SB7zxHW5paYHUdEROQnqaAEiDYxYYzumwxoFEVERLyfCkoAuWtYB6wWg6U7itmQd8zsOCIiIj9KBSWApLYM56peSUD9Hj0iIiLeSgUlwNx1YQcMAxZvKWRLfqnZcURERE5KBSXAdIiL5LIeiQA8v2SnyWlEREROTgUlAE25uCOGAYs2F7DxgMPsOCIiIj+gghKAOsZHMap3GwCeWbzd5DQiIiI/pIISoO65uCNWi8GX24vJ2XfU7DgiIiKNNElBOXjwIDfeeCMtW7YkLCyMHj16sHbt2obH3W43M2bMIDExkbCwMDIzM9m5U/MhmlNaqwiuPbEuikZRRETE23i8oBw9epShQ4cSFBTEp59+ypYtW/jLX/5CixYtGp7z5JNP8vzzzzNr1ixWrVpFREQEI0aMoKqqytNx5CfcfXEHgqwGX+86wordh82OIyIi0sBwe3h722nTpvH111/zn//856SPu91ukpKSuO+++7j//vsBcDgcxMfH88YbbzB27Nif/R6lpaXY7XYcDgfR0dGejB9wZizcxJvZ++jftgXz7xyCYRhmRxIRET91Op/fHh9B+fDDD+nfvz9jxowhLi6OPn368OqrrzY8vnfvXgoKCsjMzGw4Z7fbGTRoENnZ2Sd9zerqakpLSxsd4hmTLuxAiM3C2n1HWbqj2Ow4IiIiQBMUlD179jBz5kw6duzIZ599xq9//Wt+85vfMHv2bAAKCgoAiI+Pb/R18fHxDY99X1ZWFna7veFISUnxdOyAFR8dyk2D2wLwzOIdeHhATURE5Ix4vKC4XC769u3LY489Rp8+fZg4cSITJkxg1qxZZ/ya06dPx+FwNBx5eXkeTCx3DmtPeLCVbw84WLyl0Ow4IiIini8oiYmJdO3atdG5Ll26sH//fgASEhIAKCxs/EFYWFjY8Nj3hYSEEB0d3egQz2kVGcL4oWlA/SiKy6VRFBERMZfHC8rQoUPZvr3xbas7duygbdv6ywjp6ekkJCSwZMmShsdLS0tZtWoVQ4YM8XQcOUUTz2tPVKiNbQVlfLzxkNlxREQkwHm8oNx7772sXLmSxx57jF27djF37lz++te/MmnSJAAMw2DKlCk8+uijfPjhh2zcuJGbb76ZpKQkRo0a5ek4cors4UFMOK8dAM9+voM6p8vkRCIiEsg8XlAGDBjAggULePvtt+nevTuPPPIIzz33HOPGjWt4zoMPPsjdd9/NxIkTGTBgAOXl5SxatIjQ0FBPx5HTMH5oGjHhQewpruCD9flmxxERkQDm8XVQmoPWQWk6s5bu5vFPt5ESG8YX9w0jyKrdEERExDNMXQdFfNvNQ9rSKjKEvJLjzF97wOw4IiISoFRQpJHwYBuTLmwPwAtf7KSq1mlyIhERCUQqKPIDvxyYSqI9lEOOKt5evd/sOCIiEoBUUOQHQoOsTL6oAwAvfbmb4zUaRRERkealgiInNaZfCimxYRwur+bN7Fyz44iISIBRQZGTCrZZuOfiTkD9nT1lVbUmJxIRkUCigiI/alTvJNq1juBoZS2vf51rdhwREQkgKijyo2xWC/dm1o+ivLpsD0crakxOJCIigUIFRX7SyB6JdEmMpqy6jhe/3GV2HBERCRAqKPKTLBaD6ZdmAPBmdi55JZUmJxIRkUCggiI/6/xOrTmvYytqnW6e/vf2n/8CERGRs6SCIqfkt5dkYBiwcH0+Gw84zI4jIiJ+TgVFTkn3Nnau7t0GgMc+2YoP7jEpIiI+RAVFTtnU4Z0ItlnI3nOEr3YUmx1HRESagNvtZldRmdkxVFDk1CW3CGf8OWkAPP7JNpwujaKIiPibf317iF88u4yHF24yNYcKipyWu4Z1wB4WxPbCMv657oDZcURExIOqap08sWgbbjfERoSYmkUFRU6LPTyIyRfWbyT4zL93aCNBERE/MntFLgeOHic+OoQJ56ebmkUFRU7bTUPa0iYmjILSKl77eq/ZcURExANKKmoaFuS8f3hnwoNtpuZRQZHTFhpk5YERnQGY+dVujpRXm5xIRETO1v/7fAdlVXV0TYxmdN9ks+OooMiZubJXEt2SoimvruOFL7QEvoiIL9tdXM6cVfsB+MPILlgshsmJVFDkDFksBr+7rAsA/1i5j9zDFSYnEhGRM5X1yTbqXG4uzojjnA6tzI4DqKDIWRjaoRUXdGpNncvNU1oCX0TEJ2XvPsLnWwuxWgymn/gfT2+ggiJnZdql9Uvgf/ztIb7Zf9TsOCIichpcLjd//mQLADcMTKVDXKTJif5LBUXOSpf/mUyV9ek2LYEvIuJDFnxzkE0HS4kMsTEls6PZcRpRQZGzNvUXnQixWVi9t4QvthWZHUdERE7B8Rpnww71d13YnpaR5i7M9n0qKHLWkmLCuO3c+gV9Hv90G3VOl8mJRETk5/x9+R4OOapoExPGbUPNXZTtZFRQxCN+Paw9LcKD2FlUzns5WgJfRMSbFZVVMfOr3QA8eElnQoOsJif6IRUU8Yjo0CDuvqj++uUzi3dQWVNnciIREfkxzy7eSUWNk17Jdq7omWR2nJNSQRGPuXFwW1Jjwykqq+bVZVoCX0TEG20vKOOdNScWZbu8q1csynYyKijiMcE2Cw9ecmIJ/KW7yD923OREIiLyfY99shWXGy7plsCAtFiz4/woFRTxqJE9EhmYFktVrYusT7eZHUdERP7Hsh3FLN1RTJDVYNqlGWbH+UkqKOJRhmHw8JVdsRjw0YZ8Vu8tMTuSiIgATpebxz7ZCsBNg9NIaxVhcqKfpoIiHtctyc7YgakA/PHDzThdWrxNRMRs89fmsa2gDHtYEL+5uIPZcX6WCoo0ifuHdyY61MaWQ6XMOzEZS0REzFFRXcdfFu8A4O6LOhATHmxyop+ngiJNIjYimHt/0QmApz/bjqOy1uREIiKB65VleyguqyY1NpybhrQ1O84pUUGRJnPj4LZ0jIvkaGUtz36+w+w4IiIB6eCx4/x1Wf2ibNMuzSDE5n2Lsp2MCoo0mSCrhYev6AbAWyv3saOwzOREIiKB588fb6Gq1sXAtFgu7Z5gdpxTpoIiTercjq0Y0S0ep8vN/320Rbsdi4g0o+U7D/PJxgKsFoM/XdUNw/DORdlORgVFmtwfRnYl2GZh+a7D/HtLodlxREQCQk2di4c/3ATATYPb0iUx2uREp0cFRZpcSmw4E89rB8CjH2+hqtZpciIREf/3+td72V1cQavI/9604EtUUKRZ3HVhexKiQ8krOc7f/rPH7DgiIn6twFHF80t2AvDbSzKwhwWZnOj0qaBIswgPtjH9svpllV/6cjeHHNqnR0SkqTz2yVYqapz0SY1hdN9ks+OcERUUaTZX9kpiQFoLjtc6eVz79IiINImVe47w4YZ8DAMeuaq71+5W/HNUUKTZGIbBw1d0wzBg4fp81uZqnx4REU+qdbp4eOFmAG4YmEr3NnaTE505FRRpVt3b2Bk7IAWAP36kfXpERDzprex9bC8so0V4EA+M6Gx2nLOigiLN7v7hnYkKtbHpYCnvrs0zO46IiF8oKqvi2RP77TwwIsMn9tv5KSoo0uxaRoZwb2b9LW9PfbYdx3Ht0yMicrae+HQ7ZdV19Ey2c/2JkWpfpoIiprhpSP0+PSUVNfy/z3eaHUdExKfl7Cvhn+sOAPCnK7th9dGJsf9LBUVMEWS1MOOKrgC8mZ3L9gLt0yMiciacLjczTkyMvb5/Cn1SW5icyDNUUMQ053VszYhu8dS53PxuwUZcmjArInLa5q7ez+b8UqJDbTx4iW9PjP1fKihiqj9e2Y2IYCs5+47yjibMioiclpKKGp7+bDsA94/oTMvIEJMTeY4Kipgq0R7GfcPrG3/WJ1spLqs2OZGIiO946rNtOI7X0iUxmhsGppodx6NUUMR0t5yTRo82dkqr6nj04y1mxxER8Qkb8o4xb039yPMjV3XDZvWvj3T/+mnEJ1ktBo9d3QPLiRVml+0oNjuSiIhXc7nczFi4CbcbrunThv5psWZH8jgVFPEKPZLt3HJOGgAPLdxEVa3T3EAiIl7snbV5bDjgIDLExrQTG7H6GxUU8Rr3De9MQnQo+45U8uIXu8yOIyLilYrKqsj6ZCsAUzI7EhcVanKipqGCIl4jMsTGH6/sBsAry3azo1Bro4iIfN+fPtxCaVUdPdrYufXEyLM/UkERrzKiWzyZXeKpdbr5vdZGERFpZPGWQj7eeAirxSDrmh5+NzH2f/nvTyY+yTAM/nRVN8KDrazJPcr8HK2NIiICUFZVy0MfbALgV+el072N3eRETavJC8rjjz+OYRhMmTKl4VxVVRWTJk2iZcuWREZGMnr0aAoLC5s6iviINjFhTP1F/WaCj32yjcPlWhtFROSpz7ZTUFpF25bhTLm4k9lxmlyTFpQ1a9bwyiuv0LNnz0bn7733Xj766CPmz5/P0qVLyc/P55prrmnKKOJjbj0nja6J0TiO1/Lnj7eaHUdExFQ5+0p4a+U+ALKu7kFYsNXkRE2vyQpKeXk548aN49VXX6VFi/9uXORwOPj73//OM888w0UXXUS/fv14/fXXWbFiBStXrmyqOOJjbFYLWdf0wDBgwTcHWb7zsNmRRERMUV3n5Lf/3IjbDWP6JXNOh1ZmR2oWTVZQJk2axMiRI8nMzGx0Picnh9ra2kbnMzIySE1NJTs7+6SvVV1dTWlpaaND/F+vlBhuHtwWgD98sFFro4hIQJr51W52FZXTKjKY34/sYnacZtMkBWXevHmsW7eOrKysHzxWUFBAcHAwMTExjc7Hx8dTUFBw0tfLysrCbrc3HCkpKU0RW7zQfSM6Ex8dQu6RSl7+UmujiEhg2VlYxksnfvc9fEU3YsKDTU7UfDxeUPLy8rjnnnuYM2cOoaGeWTxm+vTpOByOhiMvT3d2BIro0CD+eEX92igzl+5mV5HWRhGRwOByuZn2/kZqnW4uzojj8p6JZkdqVh4vKDk5ORQVFdG3b19sNhs2m42lS5fy/PPPY7PZiI+Pp6amhmPHjjX6usLCQhISEk76miEhIURHRzc6JHBc0j2BizPiqHW6+d2CTbjdWhtFRPzfnNX7ydl3lIhgK4+M6o5hGGZHalYeLygXX3wxGzduZP369Q1H//79GTduXMO/BwUFsWTJkoav2b59O/v372fIkCGejiN+4Lu1UcKCrKzeW8L8tQfMjiQi0qQOOY7zxKfbAHjwkgySYsJMTtT8bJ5+waioKLp3797oXEREBC1btmw4f/vttzN16lRiY2OJjo7m7rvvZsiQIQwePNjTccRPJLcI595fdOSxT7bx6MdbOL9TaxLs/rn/hIgENrfbzUMfbKa8uo4+qTHceOJmgUBjykqyzz77LJdffjmjR4/m/PPPJyEhgffff9+MKOJDbhuaTq9kO6VVdUx//1td6hERv/TppgI+31pIkNXgidE9sVoC69LOdwy3D/6WLy0txW6343A4NB8lwOwsLGPk88upcbp48tqeXNdfd3SJiP9wVNaS+exSisuq+c1FHZg6vLPZkTzqdD6/tReP+JSO8VFMHV6/xPMjH20h/9hxkxOJiHhO1qdbKS6rpn3rCCZd1MHsOKZSQRGfM+G8dvRJjaGsuo5p72/UpR4R8QvZu48wb039MhqPj+5JiM3/l7P/KSoo4nOsFoOnx/QixGZh2Y5i3lmjdXFExLdV1Tr53YKNAIwblMqAtFiTE5lPBUV8UvvWkTwwov7a7KMfb+XA0UqTE4mInLknFm1j7+EK4qND+O2lGWbH8QoqKOKzxg9Np3/bFpRX1/Hbf+quHhHxTSt2Heb1r3MBeGJ0T6JDg8wN5CVUUMRnWS0GT17bk9AgC1/vOsKcVfvNjiQiclocx2u5f/4GoP7SzrDOcSYn8h4qKOLT2rWO5MER9cOhj32ylbwSXeoREd/xp482k++oom3LcH53WeDsVHwqVFDE5916ThoD02KprHHy4Hvf4nLpUo+IeL9Fmw7x/rqDWAx45rpeRIR4fHF3n6aCIj7PYjF4akxPwoKsZO85wj9W7TM7kojITyoqq+J3CzYBcOcF7enXVnftfJ8KiviFti0jmH5Z/aWerE+2se9IhcmJREROzu1287v3N1JSUUOXxGimZHYyO5JXUkERv3HjoLYMadeS47VOHtClHhHxUvPXHuDzrUUEWy08e30vgm36KD4ZvSviNywn7uoJD7ayem8Js7NzzY4kItJIXkklf/poMwD3De9ERoL2k/sxKijiV1Ji/zsT/ruFj0REvIHT5ea+dzdQUeNkQFoLfnVeO7MjeTUVFPE74walcm6HVlTVunhg/gacutQjIl7gteV7WZ1bQniwlb+M6Y3VYpgdyaupoIjfMQyDx0f3IDLExtp9R5m1dLfZkUQkwG0vKOOpz7YD8NDlXUltGW5yIu+ngiJ+KblFOA9f0RWAZxbvYN3+oyYnEpFAVVPn4t531lPjdHFRRhxjB6SYHcknqKCI37q2XzJX9krC6XLzm7e/obSq1uxIIhKAnl+yky2HSmkRHsTjo3tgGLq0cypUUMRvGYbBo1d3JyU2jANHj/O79zdqQ0ERaVY5+47y8le7APjz1T2Iiwo1OZHvUEERvxYdGsTzY/tgsxj869tDzM85YHYkEQkQlTV13PfuelxuuLpPGy7rkWh2JJ+igiJ+r09qC6YOr1+p8eGFm9ldXG5yIhEJBI9+vJXcI5UkRIfyxyu7mR3H56igSEC48/z2DO1Qv8rs3XO/obrOaXYkEfFj//o2n7mr9gPw9Jhe2MOCTE7ke1RQJCBYLAbPXNeb2Ihgthwq5YlPt5sdSUT8VO7hCqb9cyMAdw1rz7kdW5mcyDepoEjAiI8O5ekxPQF47eu9fLGt0OREIuJvquucTH57HeXVdQxIa8HUX2gjwDOlgiIB5aKMeG49Jw2A++d/S1FplbmBRMSvPPbxVjYdrL+l+Plf9sFm1cfsmdI7JwFn2qUZdEmMpqSihnvfXa9dj0XEIz7deIjZ2fsAeOa63iTaw0xO5NtUUCTghAZZeeGXfQgLsvL1riO8smyP2ZFExMftP1LJg+99C8AdF7Tjwow4kxP5PhUUCUgd4iL545X1S+H/5d/b+UZL4YvIGfpu3klZdR392rbg/uGdzY7kF1RQJGBd1z+FkT0TqXO5+c08LYUvImfm8U+38e0BB/aw+nknQZp34hF6FyVgGYbBY1f3oE1MGHklx/nDgk1aCl9ETstnmwt4/etcAP4yphdtYjTvxFNUUCSg1f8fT2+sFoMPN+TznpbCF5FTlFdSyQPzNwAw4bx0MrvGm5zIv6igSMDr1zaWezM7AvDQwk1sPVRqciIR8XY1dS7ufvsbSqvq6J0Sw4OXZJgdye+ooIgAvx7WgfM6tqKq1sUdb+VwrLLG7Egi4sWeXLSN9XnHiA618eINmnfSFPSOigBWi8HzY/uQ3CKM/SWV3DNvPU6tjyIiJ/H5lkL+tnwvUL/PTnKLcJMT+ScVFJETWkQE88pN/QixWVi6o5jnPt9hdiQR8TIHjx3nvhPzTm4bms7wbgkmJ/JfKigi/6Nbkp3HR/cA4IUvdvHZ5gKTE4mIt6h1urh77jocx2vplWxn2qWad9KUVFBEvufqPskN+/Xc9+4GdheXmxtIRLzCI//awrr9x4gKtfHiDX0JtukjtCnp3RU5id+P7MLAtFjKq+u4460cyqvrzI4kIiZ6e/V+3szeh2HAs9f1JiVW806amgqKyEkEWS28OK4P8dEh7Coq5/53N2gRN5EAtSa3hBkLNwFw3y86ab2TZqKCIvIj4qJCmXljP4KsBos2FzBz6W6zI4lIMzt47Dh3vpVDrdPNyJ6JTLqwg9mRAoYKishP6Jvagj9e2Q2Apz/bzrIdxSYnEpHmcrzGycQ313KkooauidE8dW1PDMMwO1bAUEER+Rk3DEzl+v4puNzwm3nfkFdSaXYkEWlibrebB97bwOb8UlpGBPPqLf0JD7aZHSugqKCI/AzDMPjTVd3olWznWGUtd7yVw/Eap9mxRKQJvfzVbv717SFsFoOZN/bTJoAmUEEROQWhQVZm3tiPlhHBbDlUyu8XbNSkWRE/9fmWQp7+93YA/nRVNwamx5qcKDCpoIicoqSYMF64oQ9Wi8H73xxk9opcsyOJiIftLCxjyjvrcbvhxsGpjBvU1uxIAUsFReQ0nNO+FdNPrB756Mdbyd59xOREIuIpjspaJry5lvLqOgalx/LwFd3MjhTQVFBETtPt56ZzZa8k6lxu7nhrLbuKysyOJCJnqc7pYvLb68g9UkmbmDBeHtdXOxSbTO++yGkyDIMnr+1J39QYSqvquPX1NRSXVZsdS0TOwuOfbuM/Ow8TFmTl1Zv70zIyxOxIAU8FReQMhJ74Jda2ZTgHjh7nV7PXUFmj5fBFfNE/cw7wt+V7AfjLdb3omhRtciIBFRSRM9YyMoQ3xg+kRXgQGw44+M3b63G6dGePiC/5Zv9Rpi/YCMBvLurAZT0STU4k31FBETkL6a0iePXm/gTbLHy+tZBH/rXF7Egicor2H6lkwps51NS5GN41nimZncyOJP9DBUXkLPVPi+WZ63oB8MaKXP5+YqhYRLzX4fJqbn5tFYfLq+mSGM0z1/fGYtEy9t5EBUXEAy7vmcS0htuPt7BoU4HJiUTkx1RU13H7G2vIPVJJcoswZo8fQGSIlrH3NiooIh5yx/ntGDcoFbcb7pn3Dd/sP2p2JBH5nlqni1/PWceGAw5iI4J587aBxEWHmh1LTkIFRcRDDMPgT1d248LOramuc/Gr2WvZf0QbC4p4C7fbzW/f+5ZlO4oJC7Ly91v60651pNmx5EeooIh4kM1q4cUb+tItKZojFTXc+sZqjlXWmB1LRIAnFm3n/W8OYrUYvDyuL31SW5gdSX6CCoqIh0WE2Hjt1gEk2UPZU1zBxDdzqK7T7sciZnpt+V5mLd0NwOPX9ODCjDiTE8nPUUERaQLx0aG8Pn4gUSE2VueW8MD8b3FpjRQRU3y0IZ9HPq5fAuCBEZ0Z0z/F5ERyKlRQRJpI54QoZt7YD5vF4MMN+Tx1Yvt2EWk+K3Yd5r53N+B2wy1D2nLXsPZmR5JT5PGCkpWVxYABA4iKiiIuLo5Ro0axfXvjX8xVVVVMmjSJli1bEhkZyejRoyksLPR0FBHTnduxFVnX9ABg5le7mfnVbpMTiQSOzfkOJr6VQ43TxWU9EphxRTcMQ2ud+AqPF5SlS5cyadIkVq5cyeLFi6mtrWX48OFUVFQ0POfee+/lo48+Yv78+SxdupT8/HyuueYaT0cR8Qpj+qfw4CWdAXhi0TZe00JuIk0ur6SSW19fQ3l1HYPSY3nmut5YtRCbTzHcbneTXhgvLi4mLi6OpUuXcv755+NwOGjdujVz587l2muvBWDbtm106dKF7OxsBg8e/LOvWVpait1ux+FwEB2tTZ3ENzzz7+08/8UuAB67ugc3DEo1OZGIfzpSXs2YWdnsOVxBRkIU7945hOjQILNjCaf3+d3kc1AcDgcAsbGxAOTk5FBbW0tmZmbDczIyMkhNTSU7O7up44iY5t5fdGLi+e0A+P0HG3l/3QGTE4n4n8qaOm6bvZY9hytoExPG7NsGqpz4qCZd29flcjFlyhSGDh1K9+7dASgoKCA4OJiYmJhGz42Pj6eg4OTLg1dXV1NdXd3w59LS0ibLLNJUDMNg+qUZVNc6mZ29j/vnbyDYZuHynklmRxPxC5U1dYx/fQ0b8o4REx7E7NsGEq9VYn1Wk46gTJo0iU2bNjFv3ryzep2srCzsdnvDkZKiW8TENxmGwcNXdGPsgBRcbpgybz3/3qx9e0TO1nflZNXeEqJCbLx+6wA6xGmVWF/WZAVl8uTJ/Otf/+LLL78kOTm54XxCQgI1NTUcO3as0fMLCwtJSEg46WtNnz4dh8PRcOTl5TVVbJEmZ7EY/PnqHozqnUSdy83kud+wdEex2bFEfNb3y8ns2wdqlVg/4PGC4na7mTx5MgsWLOCLL74gPT290eP9+vUjKCiIJUuWNJzbvn07+/fvZ8iQISd9zZCQEKKjoxsdIr7MajF4ekwvLu2eQI3TxcQ315K9+4jZsUR8zsnKSV+VE7/g8YIyadIk/vGPfzB37lyioqIoKCigoKCA48ePA2C327n99tuZOnUqX375JTk5OYwfP54hQ4ac0h08Iv7CZrXw/8b24eKMOKrrXNw+ew05+0rMjiXiM1RO/JvHbzP+sUVwXn/9dW699VagfqG2++67j7fffpvq6mpGjBjByy+//KOXeL5PtxmLP6mqdTLhzbX8Z+dhokJszJkwiJ7JMWbHEvFqKie+6XQ+v5t8HZSmoIIi/uZ4jZNbXl/N6r0l2MOCmDdxMF0S9Xdb5GRUTnyXV62DIiI/LyzYymu3DqB3SgyO47Xc+LdV7CwsMzuWiNdROQkcKigiXiIyxMbs2wbSLSmaIxU1XPdKNuvzjpkdS8RrqJwEFhUUES9iDwviH7cPoldKDEcra7nh1ZV8veuw2bFETKdyEnhUUES8TIuIYOb8ahBDO7SkssbJ+NfXsGjTIbNjiZhG5SQwqaCIeKHIEBuv3TqgYZ2Uu+asY97q/WbHEml2juO13PqaykkgUkER8VIhNisv3tC3YVn8ae9vZNbS3WbHEmk2+ceOM2bWClbnqpwEIhUUES9mtRhkXdODOy9oD8Djn24j65Ot+ODqACKnZVtBKde8vIIdheXER4fwzh1DVE4CjAqKiJczDINpl2Yw/dIMAF5Ztodp/9xIndNlcjKRprFi12HGzMymoLSKjnGRvH/XULomaV2gQKOCIuIj7rigPU+O7onFgHfW5jF57jdU1TrNjiXiUQvXH+SW11dTVl3HwPRY3rvzHNrEhJkdS0yggiLiQ64bkMLL4/oRbLWwaHMBt72xhvLqOrNjiZw1t9vNK0t3c8+89dQ63Yzsmcibtw3EHh5kdjQxiQqKiI+5pHsCb9w2gIhgKyt2H+GGV1dSUlFjdiyRM+Z0ufnTR1vI+nQbALefm84LY/sQGmQ1OZmYSQVFxAed074Vb08cTGxEMN8ecHDtrBXsPVxhdiyR01ZV62TSnHW8sSIXgD+M7MJDl3fFYjn5xrMSOFRQRHxUz+QY3r1jCEn2UPYUV3DVi8tZuqPY7Fgip+xoRQ03/m0VizYXEGy18OINffjVee3MjiVeQgVFxId1iIvkg8lD6de2BaVVdYx/fTWvLN2t25DF6+WVVDJ61grW7jtKdKiNN28fyOU9k8yOJV5EBUXEx8VFhTJ3wiCu71+/oFvWp9uY8s563eEjXmtD3jGumbmCPcUVJNlDee/X5zC4XUuzY4mXUUER8QMhNiuPj+7B/13VDZvFYOH6fK6dtYKDx46bHU2kgdvtZs6qfYyZlU1xWTUZCVG8f9dQOsVHmR1NvJAKioifMAyDm4ek8dbtg4iNCGbTwVKuenE5a3JLzI4mwvEaJ/fN38DvF2yixulieNd43r1zCAn2ULOjiZdSQRHxM0Pat+TDyUPpkhjN4fIabnh1JXNW7TM7lgSwvYcruPrlr3l/3UGsFoPpl2bwyk39iA7VGify41RQRPxQcotw/vnrIYzsmUit083vF2zi9ws2UlOn5fGleX22uYArX1jOtoIyWkWGMOdXg7jjgvYYhm4jlp+mgiLip8KDbbz4yz48MKIzhgFzVu1n3N9WUlxWbXY0CQB1ThdZn27ljrdyKKuuY0BaCz75zbmaDCunTAVFxI8ZhsGkCzvw91v6ExViY03uUa58cTnr846ZHU38WFFZFeP+topXlu4BYMJ56cydMJi4aM03kVOngiISAC7KiGfBpKG0axXBIUcVo2eu4P99vlM7IovHrckt4fLnl7NqbwmRITZeHteX34/sSpBVHzdyevQ3RiRAdIiLZMGkoYzsmYjT5ebZz3cw5pVscrVEvniA2+3mb//Zw9i/rqSorJpO8ZEsnDyUy3okmh1NfJQKikgAsYcF8eIv+/Dc9b2JCrXxzf5jXPb8f3h79X6tPitnrKSihl//Yx2PfrwVp8vNVb2T+GDSUNq3jjQ7mvgww+2Dv5VKS0ux2+04HA6io6PNjiPikw4eO859765n5Z76dVIyu8SRdU1PWkeFmJxMfMnH3x5ixsJNHKmoIchqMOPyrtw4uK3u0pGTOp3PbxUUkQDmcrn5+/K9PPXZdmqcLlpGBPP46J78omu82dHEyxWXVTNj4SY+3VQAQOf4KJ4e04seyXaTk4k3U0ERkdOy9VAp976znm0FZQCMHZDCQ5d3JSLEZnIy8TZut5uF6/P540ebOVZZi81icNeFHZh8YQeCbZo1ID9NBUVETltVrZNnFu/g1f/swe2Gti3Deea63vRr28LsaOIlikqr+N2CTXy+tRCAronRPDWmJ92SNGoip0YFRUTOWPbuI9z37nryHVVYDLhrWAcmX9SB0CCr2dHEJG63m3+uO8j/fbSZ0qo6gqwGv7moI3cOa6/bh+W0qKCIyFlxHK/ljx9uZsE3BwFIiQ3jDyO7MrxrvCY/BphDjuNMf38jX20vBqBnsp2nru1F5wTtQCynTwVFRDzik42H+L+PtlBQWgXAuR1a8fAVXekYrw8nf+d2u5m3Jo/HPt5KWXUdwTYL92Z2YsJ56dg0aiJnSAVFRDymorqOmV/t5q/L9lDjdGG1GNwyJI17MjtiD9NutP4oZ18Jj32yjZx9RwHokxrDU9f2pEOciqmcHRUUEfG4fUcqePTjrSzeUj9BsmVEMA+M6MyY/ilYLbrs4w/2FJfz5KLtLNpcf+twaJCF+4d3ZvzQdP03Fo9QQRGRJrNsRzF/+mgzu4vrl8jv0cbOH6/sSr+2sSYnkzNVXFbN80t2Mnf1fpwuNxYDruufwr2/6ES8NvgTD1JBEZEmVet08Wb2Pp5bvIOy6joAru7ThmmXZugDzYdU1tTxt//s5ZWlu6mocQJwcUYcv700g06aZyRNQAVFRJrF4fJqnlq0nXdz8nC7ITzYyoTz2nHrOWm0iAg2O578iDqni/k5B3h28Q6KyqoB6JVsZ/plXRjcrqXJ6cSfqaCISLP69sAx/vjhZtbtPwbUF5VfDkzlV+elk2gPMzecNHC73SzZWsTji7axq6gcgNTYcB68pDMjeyTqFnJpciooItLs3G43n2ws4KUvd7HlUCkAQVaDa/okc8cF7WinnW1N43S5+WJbEa8u28Pq3PrNIVuEB3H3RR0ZNziVEJsW4ZPmoYIiIqZxu90s3VHMzK92s2pv/YehYcCl3RP49QUdtJlcMyqrquXdtQeYvSKX/SWVAITYLNx2bjp3XtBet4lLs1NBERGvkLPvKDO/2sXnW4sazp3XsRW/HtaeIe1a6pJCE9l7uILZK3KZvzavYfJrdKiNXw5M5dahabrsJqZRQRERr7KtoJRXlu7hww35OF31v3J6p8Rw5wXtyewSp5VJPcDtdrN812Fe/zqXL7cX8d1v9g5xkdx6ThrX9G1DeLB2pxZzqaCIiFfKK6nkr8v28O7aPKrrXED9gm9X9EpiVJ829Eq2a1TlNB2vcfL+Nwd44+tcdp6Y+ApwYefWjB+aznkdW+k9Fa+hgiIiXq24rJrXv97LO2vyOFJR03A+vVUEV/VOYlTvNqS1ijAxoXerc7pYnVvCok0FLFyfj+N4LQARwVau7ZfMLeekaVKyeCUVFBHxCbVOF8t3HeaDbw7y2eYCqmpdDY/1SY1hVO82XN4zkZaRISam9A7VdU6+3nWYRZsKWLylkKOVtQ2PpcSGccuQNK4bkEJ0qCa+ivdSQRERn1NeXce/Nxfwwfp8lu8s5sRUFawWg/M7tmJUnzZkdoknIiRw5lFUVNfx1fZiFm0u4MttRZSfWLUX6m8T/kXXeC7tnsj5nVprrxzxCSooIuLTisqq+GjDIRauP8i3BxwN520Wg+5t7AxqF8vg9Jb0S2vhdyMGxyprWLK1iEWbC1i2o7hhrg5AfHQIl3RLYET3BAamxWpysfgcFRQR8Ru7ispZuP4gH27IZ9+RykaPWQzomhTNoPSWDEqPZWB6LDHhvrPEfnWdk62Hyvj2wDE25Dn49sAxdhWX87+/ldu2DOeS7glc0i2BXskxWDRSIj5MBUVE/FJeSSWr9paweu8RVu0t+UFhAchIiGJQeiwD0mNp3zqSlNhwIr3gspDT5WZXUTkb8o6x4cAxvj3gYFtBKbXOH/4K7hwfxSXdE7i0RwKd46N0F474DRUUEQkIBY4qVp0oK6v2HGF3ccVJnxcbEUxKbDipseGkxoaRGhtOSotwUmLDSbSHeuRSyfEaJ8Vl1RSXV9X/87ujvJrdRRVsyndQeWLRtO9n65lsp2dyDL1O/LN1lCYFi39SQRGRgFRcVs3qEyMs6/OOsb+kstHdLidjsxi0aRFGbEQwQRYLNquBzWrBZjGwWQyCrCfOWU6csxpYDIOSyhqKy6o5fKKIlP3PBNYfExFspXsbO71SYuiZbKdXcgzJLcI0QiIBQwVFROSE0qpa8koqySs5Tl5JJftPHHkllRw4epwap+vnX+QUhdgsxEWH0DoyhNZRJ47IUJJbhNEz2U671pG620YC2ul8fpt/YVZEpAlFhwbRLclOt6QfblLocrkpLKti35FKSo/XUudyU+t0Ued0U+dyUet04/zunMtNnbP+nMvtpkV48H9LyIkjKsSm0RARD1FBEZGAZbEYJNrDtHmeiBfSTfQiIiLidVRQRERExOuooIiIiIjXUUERERERr6OCIiIiIl5HBUVERES8jqkF5aWXXiItLY3Q0FAGDRrE6tWrzYwjIiIiXsK0gvLOO+8wdepUHn74YdatW0evXr0YMWIERUVFZkUSERERL2FaQXnmmWeYMGEC48ePp2vXrsyaNYvw8HBee+01syKJiIiIlzCloNTU1JCTk0NmZuZ/g1gsZGZmkp2d/YPnV1dXU1pa2ugQERER/2VKQTl8+DBOp5P4+PhG5+Pj4ykoKPjB87OysrDb7Q1HSkpKc0UVERERE/jEXTzTp0/H4XA0HHl5eWZHEhERkSZkymaBrVq1wmq1UlhY2Oh8YWEhCQkJP3h+SEgIISEhzRVPRERETGZKQQkODqZfv34sWbKEUaNGAeByuViyZAmTJ0/+2a93u90AmosiIiLiQ7773P7uc/ynmFJQAKZOncott9xC//79GThwIM899xwVFRWMHz/+Z7+2rKwMQHNRREREfFBZWRl2u/0nn2NaQbn++uspLi5mxowZFBQU0Lt3bxYtWvSDibMnk5SURF5eHlFRURiG0QxpvV9paSkpKSnk5eURHR1tdhy/p/e7+ek9b156v5tfILznbrebsrIykpKSfva5hvtUxlnE65WWlmK323E4HH77F9ub6P1ufnrPm5fe7+an97wxn7iLR0RERAKLCoqIiIh4HRUUPxESEsLDDz+s27Gbid7v5qf3vHnp/W5+es8b0xwUERER8ToaQRERERGvo4IiIiIiXkcFRURERLyOCoqIiIh4HRUUP1ZdXU3v3r0xDIP169ebHcdv5ebmcvvtt5Oenk5YWBjt27fn4YcfpqamxuxofuOll14iLS2N0NBQBg0axOrVq82O5LeysrIYMGAAUVFRxMXFMWrUKLZv3252rIDx+OOPYxgGU6ZMMTuK6VRQ/NiDDz54SssJy9nZtm0bLpeLV155hc2bN/Pss88ya9Ysfve735kdzS+88847TJ06lYcffph169bRq1cvRowYQVFRkdnR/NLSpUuZNGkSK1euZPHixdTW1jJ8+HAqKirMjub31qxZwyuvvELPnj3NjuId3OKXPvnkE3dGRoZ78+bNbsD9zTffmB0poDz55JPu9PR0s2P4hYEDB7onTZrU8Gen0+lOSkpyZ2VlmZgqcBQVFbkB99KlS82O4tfKysrcHTt2dC9evNh9wQUXuO+55x6zI5lOIyh+qLCwkAkTJvDWW28RHh5udpyA5HA4iI2NNTuGz6upqSEnJ4fMzMyGcxaLhczMTLKzs01MFjgcDgeA/j43sUmTJjFy5MhGf9cDnWm7GUvTcLvd3Hrrrdx5553079+f3NxcsyMFnF27dvHCCy/w9NNPmx3F5x0+fBin0/mDXc7j4+PZtm2bSakCh8vlYsqUKQwdOpTu3bubHcdvzZs3j3Xr1rFmzRqzo3gVjaD4iGnTpmEYxk8e27Zt44UXXqCsrIzp06ebHdnnnep7/r8OHjzIJZdcwpgxY5gwYYJJyUU8Y9KkSWzatIl58+aZHcVv5eXlcc899zBnzhxCQ0PNjuNVtNS9jyguLubIkSM/+Zx27dpx3XXX8dFHH2EYRsN5p9OJ1Wpl3LhxzJ49u6mj+o1Tfc+Dg4MByM/PZ9iwYQwePJg33ngDi0X9/2zV1NQQHh7Oe++9x6hRoxrO33LLLRw7doyFCxeaF87PTZ48mYULF7Js2TLS09PNjuO3PvjgA66++mqsVmvDOafTiWEYWCwWqqurGz0WSFRQ/Mz+/fspLS1t+HN+fj4jRozgvffeY9CgQSQnJ5uYzn8dPHiQCy+8kH79+vGPf/wjYH+hNIVBgwYxcOBAXnjhBaD+skNqaiqTJ09m2rRpJqfzP263m7vvvpsFCxbw1Vdf0bFjR7Mj+bWysjL27dvX6Nz48ePJyMjgt7/9bUBfWtMcFD+Tmpra6M+RkZEAtG/fXuWkiRw8eJBhw4bRtm1bnn76aYqLixseS0hIMDGZf5g6dSq33HIL/fv3Z+DAgTz33HNUVFQwfvx4s6P5pUmTJjF37lwWLlxIVFQUBQUFANjtdsLCwkxO53+ioqJ+UEIiIiJo2bJlQJcTUEEROWuLFy9m165d7Nq16wclUAOUZ+/666+nuLiYGTNmUFBQQO/evVm0aNEPJs6KZ8ycOROAYcOGNTr/+uuvc+uttzZ/IAlYusQjIiIiXkez+ERERMTrqKCIiIiI11FBEREREa+jgiIiIiJeRwVFREREvI4KioiIiHgdFRQRERHxOiooIiIi4nVUUERERMTrqKCIiIiI11FBEREREa+jgiIiIiJe5/8DpiKQovjNVXQAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "id": "8cc5601b2bfbdacf",
      "metadata": {
        "id": "8cc5601b2bfbdacf"
      },
      "source": [
        "What is the gradient of f(x) at x=2?\n",
        "- Human's solution:\n",
        "    - f'(x) = 6x - 4\n",
        "    - f'(2) = 6*2 - 4 = 8\n",
        "\n",
        "- Computer's solution:\n",
        "    - f'(2) = (f(2 + h) - f(2)) / h\n",
        "        - h: a small number (0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "id": "a4858fe8eb81441b",
      "metadata": {
        "id": "a4858fe8eb81441b"
      },
      "source": [
        "def gradient(function, x, _h=0.0001):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    grad = (function(x+_h)-function(x))/_h\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return grad"
      ],
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "id": "6c0972b63c72fe7c",
      "metadata": {
        "id": "6c0972b63c72fe7c",
        "outputId": "0ac889b0-544b-4922-9744-0962b9b0492b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "gradient(f, 2)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.000300000023941"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "id": "c9d3eeb8dd980e7",
      "metadata": {
        "id": "c9d3eeb8dd980e7"
      },
      "source": [
        "Let's calculate the gradient of a more complex function.\n",
        "\n",
        "![(x + y) * z](https://github.com/qorjiwon/LLM101n/blob/master/assets/(x+y)z.png?raw=1)\n",
        "\n",
        "(Image credit: Stanford CS231n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ad426cfc324aeb",
      "metadata": {
        "id": "34ad426cfc324aeb"
      },
      "source": [
        "#### Example 2: Multi-Variable Function"
      ]
    },
    {
      "cell_type": "code",
      "id": "7c8986c366d8e99d",
      "metadata": {
        "id": "7c8986c366d8e99d"
      },
      "source": [
        "def f(x, y, z):\n",
        "    out = (x + y) * z\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "id": "25857fe002e61146",
      "metadata": {
        "id": "25857fe002e61146",
        "outputId": "5ad4dc1b-7076-40c5-82ba-27e3e40d2e32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "f(-2, 5, -4)  # f(-2, 5, -4) = (-2 + 5) * -4 = -12"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-12"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "execution_count": 8
    },
    {
      "cell_type": "markdown",
      "id": "a9171fd855b2b1b7",
      "metadata": {
        "id": "a9171fd855b2b1b7"
      },
      "source": [
        "Let's calculate the gradient of this function\n",
        "\n",
        "**Goal**:\n",
        "1. df/dx at x=-2\n",
        "2. df/dy at y=5\n",
        "3. df/dz at z=-4"
      ]
    },
    {
      "cell_type": "code",
      "id": "f2c53484de26a8ac",
      "metadata": {
        "id": "f2c53484de26a8ac",
        "outputId": "7dc8566f-0ec9-4ed3-8816-a3cbe7609d3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get the derivative of f(x, y, z) with respect to x, y, z\n",
        "h = 0.0001\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to x=2                                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "x = -2\n",
        "y = 5\n",
        "z = -4\n",
        "df_dx = (f(x+h, y, z) - f(x, y, z))/h\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dx: {df_dx}\")\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to y=-3                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "df_dy = (f(x, y+h, z) - f(x, y, z))/h\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dy: {df_dy}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to z=10                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "df_dz = (f(x, y, z+h) - f(x, y, z))/h\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dz: {df_dz}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df/dx: -3.9999999999906777\n",
            "df/dy: -3.9999999999906777\n",
            "df/dz: 3.000000000010772\n"
          ]
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "markdown",
      "id": "b4fecd506dd71a85",
      "metadata": {
        "id": "b4fecd506dd71a85"
      },
      "source": [
        "This time, let's calculate the gradient using the chain rule.\n",
        "\n",
        "\n",
        "![chain rule](https://github.com/qorjiwon/LLM101n/blob/master/assets/chain_rule.png?raw=1)\n",
        "\n",
        "(Image credit: Stanford CS231n)\n",
        "\n",
        "**Chain Rule**:\n",
        "\n",
        "q = x + y\n",
        "\n",
        "f = q * z\n",
        "\n",
        "- df/dx = df/dq * dq/dx\n",
        "- df/dy = df/dq * dq/dy"
      ]
    },
    {
      "cell_type": "code",
      "id": "26df18dea033aadf",
      "metadata": {
        "id": "26df18dea033aadf"
      },
      "source": [
        "# Get the derivative of f(x, y, z) with respect to x, y using the chain rule\n",
        "\n",
        "# Redefine the function f(x, y, z) and q(x, y)\n",
        "def f(q, z):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    out = q * z\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return out\n",
        "\n",
        "def q(x, y):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    out = x + y\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": 10
    },
    {
      "cell_type": "code",
      "id": "cb02602405abfbf8",
      "metadata": {
        "id": "cb02602405abfbf8",
        "outputId": "1ead580b-15b2-40f9-afb1-a1fd2ed53f9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# q\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to q=-2                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "df_dq = ((f(q(-2, 5) + h, -4) - f(q(-2, 5), -4))/h)\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dq: {df_dq}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df/dq: -4.000000000008441\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "id": "3bf19d2256e21a37",
      "metadata": {
        "id": "3bf19d2256e21a37",
        "outputId": "628414ee-33cb-43a6-afe9-37249bb9a2dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# x\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of q with respect to x=-2                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "dq_dx = (q(-2+h, 5) - q(-2, 5))/h\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"dq/dx: {dq_dx}\")\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to x=-2                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "df_dx = df_dq * dq_dx\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dx: {df_dx}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dq/dx: 0.9999999999976694\n",
            "df/dx: -3.999999999999119\n"
          ]
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "id": "22816305e040df95",
      "metadata": {
        "id": "22816305e040df95",
        "outputId": "95ad836b-8823-4f46-f01a-c68ecb4a5d03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# y\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of q with respect to y=5                                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "dq_dy = (q(-2, 5+h) - q(-2, 5))/h\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"dq/dy: {dq_dy}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to y=5                                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "df_dy = df_dq * dq_dy\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dy: {df_dy}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dq/dy: 0.9999999999976694\n",
            "df/dy: -3.999999999999119\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "markdown",
      "id": "a1dd5a4407648515",
      "metadata": {
        "id": "a1dd5a4407648515"
      },
      "source": [
        "### Custom Auto-Grad Engine\n",
        "\n",
        "Neural networks are a series of functions that are composed together. Each function is a layer in the network. To get the gradient, we need to calculate the derivative of each function using the chain rule.\n",
        "\n",
        "**Let's make a custom tensor object that calculates and stores the gradient of the tensor.**\n",
        "1. Support basic operations: addition, multiplication, tanh, power\n",
        "2. Calculate and store the gradient of the tensor\n",
        "\n",
        "- Why not use numpy arrays?:\n",
        "    - We need to keep track of the operations and tensors that lead to this tensor in order to calculate the gradient.\n",
        "\n",
        "- Numerical vs Analytical\n",
        "    - Numerical differentiation: estimates the gradient using the finite difference approximation\n",
        "        -  f'(x) = (f(x + h) - f(x)) / h\n",
        "        - Slow\n",
        "        - Precision issues\n",
        "    - Analytical differentiation: derives the function symbolically using the chain rule\n",
        "        - Fast\n",
        "        - Precise\n",
        "        - Need to code the derivative of every operation"
      ]
    },
    {
      "cell_type": "code",
      "id": "b279b1065a769ef0",
      "metadata": {
        "id": "b279b1065a769ef0"
      },
      "source": [
        "# Example usage\n",
        "# a = Tensor(-2.0)\n",
        "# b = Tensor(5.0)\n",
        "# c = Tensor(-4.0)\n",
        "# f = (a + b) * c\n",
        "# f.backward()\n",
        "\n",
        "# print(f\"a: {a}\")               # a: tensor=(-2.0)\n",
        "# print(f\"output: {f}\")          # output: tensor=12.0\n",
        "# print(f\"df/da: {a.gradient}\")  # df/da: 1.0\n",
        "# print(f\"df/db: {b.gradient}\")  # df/db: 1.0\n",
        "# print(f\"df/dc: {c.gradient}\")  # df/dc: 3.0"
      ],
      "outputs": [],
      "execution_count": 33
    },
    {
      "cell_type": "markdown",
      "id": "71703911d4435375",
      "metadata": {
        "id": "71703911d4435375"
      },
      "source": [
        "#### Tensor version 1\n",
        "\n",
        "A simple tensor object that supports addition and multiplication."
      ]
    },
    {
      "cell_type": "code",
      "id": "b961d923e295de1f",
      "metadata": {
        "id": "b961d923e295de1f"
      },
      "source": [
        "class TensorV1:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    # method to print the tensor\n",
        "    def __repr__(self):\n",
        "        return f\"tensorV1=({self.data})\"\n",
        "\n",
        "    # method to add two tensors\n",
        "    def __add__(self, other):  # self + other\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the addition operation.                                            #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output = TensorV1(self.data + other.data)\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output\n",
        "\n",
        "    # method to multiply two tensors\n",
        "    def __mul__(self, other):  # self * other\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the multiplication operation.                                      #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output = TensorV1(self.data * other.data)\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": 48
    },
    {
      "cell_type": "code",
      "id": "79bced7cd35204a8",
      "metadata": {
        "id": "79bced7cd35204a8",
        "outputId": "ffd23830-114b-46b2-ce64-50d43fdaebfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example usage\n",
        "a = TensorV1(-2.0)\n",
        "b = TensorV1(5.0)\n",
        "c = TensorV1(-4.0)\n",
        "f = (a + b) * c\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"output: {f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensorV1=(-2.0)\n",
            "output: tensorV1=(-12.0)\n"
          ]
        }
      ],
      "execution_count": 49
    },
    {
      "cell_type": "markdown",
      "source": [
        "1ï¸âƒ£ ì—­ì „íŒŒ(backpropagation)\n",
        "\n",
        "ìš°ë¦¬ëŠ” ì‹ ê²½ë§ì˜ í•™ìŠµ ê³¼ì •ì—ì„œ ì†ì‹¤ í•¨ìˆ˜(loss) ë¥¼ ì¤„ì´ëŠ” ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜(weight)ë¥¼ ì—…ë°ì´íŠ¸í•´ì•¼í•œë‹¤.\n",
        "ì´ë¥¼ ìœ„í•´ì„œëŠ” ê° ë³€ìˆ˜(í…ì„œ)ì˜ ê¸°ìš¸ê¸°(gradient, ë¯¸ë¶„ê°’) ë¥¼ ì•Œì•„ì•¼ í•˜ê³ ,\n",
        "ì´ **ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •**ì´ ë°”ë¡œ **ì—­ì „íŒŒ(backpropagation)**.\n",
        "\n",
        "2ï¸âƒ£ ë§ì…ˆ ì—°ì‚°ì˜ ì—­ì „íŒŒ\n",
        "ìš°ë¦¬ê°€ ë§ì…ˆì„ ìˆ˜í–‰í•  ë•Œ:\n",
        "\n",
        "ð‘ = ð‘Ž + ð‘\n",
        "ë¼ê³  í•˜ë©´, cì˜ ë³€í™”ëŸ‰(output.gradient)ì´ aì™€ bì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ ìƒê°í•´ë³´ìž.\n",
        "\n",
        "cê°€ ì¡°ê¸ˆ ì¦ê°€í•˜ë©´ aë„ ë˜‘ê°™ì´ ì¦ê°€í•˜ê³ , bë„ ë˜‘ê°™ì´ ì¦ê°€í•  ê²ƒì´ë‹¤.\n",
        "ì¦‰, aì— ëŒ€í•œ cì˜ ë³€í™”ìœ¨ì€ 1, bì— ëŒ€í•œ cì˜ ë³€í™”ìœ¨ë„ 1ì´ë‹¤.\n",
        "ìˆ˜ì‹ìœ¼ë¡œ ë³´ë©´:\n",
        "\n",
        "ð‘‘ð‘/ð‘‘ð‘Ž = 1, ð‘‘ð‘/ð‘‘ð‘ = 1\n",
        "\n",
        "ì´ì œ ì†ì‹¤ ð¿ ì— ëŒ€í•œ ë¯¸ë¶„ì„ ìƒê°í•´ ë³´ë©´:\n",
        "\n",
        "ð‘‘ð¿ð‘‘ð‘Ž = ð‘‘ð¿/ð‘‘ð‘ â‹… ð‘‘ð‘/ð‘‘ð‘Ž = output.gradientÃ—1\n",
        "\n",
        "ð‘‘ð¿ð‘‘ð‘ = ð‘‘ð¿/ð‘‘ð‘ â‹… ð‘‘ð‘/ð‘‘ð‘ = output.gradientÃ—1\n",
        "\n",
        "ì¦‰, c.gradientê°€ aì™€ bì˜ gradientë¡œ ê·¸ëŒ€ë¡œ ì „ë‹¬ë¼ì•¼í•œë‹¤.\n",
        "ê·¸ëž˜ì„œ += output.gradient * 1ì„ í•˜ëŠ” ê²ƒ\n"
      ],
      "metadata": {
        "id": "4o2LmqeHjEe7"
      },
      "id": "4o2LmqeHjEe7"
    },
    {
      "cell_type": "markdown",
      "id": "37036ba643a2220c",
      "metadata": {
        "id": "37036ba643a2220c"
      },
      "source": [
        "#### Tensor version 2"
      ]
    },
    {
      "cell_type": "code",
      "id": "ff003f5d07d0e5dd",
      "metadata": {
        "id": "ff003f5d07d0e5dd"
      },
      "source": [
        "class TensorV2:\n",
        "    def __init__(self, data, _children=(), _operation=''):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)  # _children: tensors that lead to this tensor (ex: 2 * 3 = 6, 2 and 3 are children of 6)\n",
        "        self.gradient = 0\n",
        "        self._backward = lambda: None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"tensorV2=({self.data})\"\n",
        "\n",
        "    def __add__(self, other):  # self + other\n",
        "        output = TensorV2(self.data + other.data, (self, other))\n",
        "        def _backward():\n",
        "            ################################################################################\n",
        "            # TODO:                                                                        #\n",
        "            # Implement the backward pass for addition.                                    #\n",
        "            # hint: use the chain rule (df/dx = df/dq * dq/dx, q = x + y)                  #\n",
        "            # df/dq -> output.gradient                                                     #\n",
        "            ################################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            self.gradient += output.gradient * 1\n",
        "            other.gradient = output.gradient * 1\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def __mul__(self, other):  # self * other\n",
        "        output = TensorV2(self.data * other.data, (self, other))\n",
        "        def _backward():\n",
        "            ################################################################################\n",
        "            # TODO:                                                                        #\n",
        "            # Implement the backward pass for multiplication.                              #\n",
        "            # hint: use the chain rule (df/dx = df/dq * dq/dx, q = x * y)                  #\n",
        "            # df/dq -> output.gradient                                                     #\n",
        "            ################################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "            self.gradient += output.gradient * other.data\n",
        "            other.gradient = output.gradient * self.data\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    # method to calculate the gradient\n",
        "    # Goes through the graph in reverse topological order and calculate the gradient\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            node._backward()"
      ],
      "outputs": [],
      "execution_count": 47
    },
    {
      "cell_type": "code",
      "id": "17b5a3f0347949",
      "metadata": {
        "id": "17b5a3f0347949",
        "outputId": "a3325795-19a4-4411-b4ed-64ab843fd867",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example usage\n",
        "a = TensorV2(-2.0)\n",
        "b = TensorV2(5.0)\n",
        "c = TensorV2(-4.0)\n",
        "f = (a + b) * c\n",
        "f.backward()\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"output: {f}\")\n",
        "print(f\"df/da: {a.gradient}\")\n",
        "print(f\"df/db: {b.gradient}\")\n",
        "print(f\"df/dc: {c.gradient}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensorV2=(-2.0)\n",
            "output: tensorV2=(-12.0)\n",
            "df/da: -4.0\n",
            "df/db: -4.0\n",
            "df/dc: 3.0\n"
          ]
        }
      ],
      "execution_count": 50
    },
    {
      "cell_type": "markdown",
      "id": "eb4b152ef95d1cd2",
      "metadata": {
        "id": "eb4b152ef95d1cd2"
      },
      "source": [
        "#### Tensor final version"
      ]
    },
    {
      "cell_type": "code",
      "id": "65624f2a4df6d632",
      "metadata": {
        "id": "65624f2a4df6d632"
      },
      "source": [
        "class Tensor:\n",
        "    def __init__(self, data, _children=(), _operation='', label=''):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)\n",
        "        self.gradient = 0\n",
        "        self._backward = lambda: None\n",
        "        # for visualization\n",
        "        self._operation = _operation  # _operation: operation that lead to this tensor (ex: 2 * 3 = 6, * is the operation)\n",
        "        self.label = label  # label: name of the tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"tensor=({self.data})\"\n",
        "\n",
        "    def __add__(self, other):  # self + other\n",
        "        output = Tensor(self.data + other.data, (self, other), '+')\n",
        "        def _backward():\n",
        "            self.gradient += 1 * output.gradient\n",
        "            other.gradient += 1 * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def __mul__(self, other):  # self * other\n",
        "        output = Tensor(self.data * other.data, (self, other), '*')\n",
        "        def _backward():\n",
        "            self.gradient += other.data * output.gradient\n",
        "            other.gradient += self.data * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    # Activation function\n",
        "    def tanh(self):  # tanh(self)\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the tanh operation.                                                #\n",
        "        # hint: use math.tanh                                                          #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output = Tensor(math.tanh(self.data))\n",
        "        def _backward():\n",
        "          self.gradient += ( math.sech(self.data) ** 2 ) * output.gradient\n",
        "        output._backward += _backward\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output\n",
        "\n",
        "    def __pow__(self, power):  # self ** power\n",
        "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the power operation.                                               #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output = Tensor(self.data ** power)\n",
        "        def _backward():\n",
        "          self.gradient += power * (self.data ** (power - 1)) * output.gradient\n",
        "        output.backward += _backward\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * Tensor(-1.0)\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)"
      ],
      "outputs": [],
      "execution_count": 42
    },
    {
      "cell_type": "code",
      "id": "fac571f40884fec5",
      "metadata": {
        "id": "fac571f40884fec5",
        "outputId": "0d833d30-6cc6-46ed-c6a2-0506031c2231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Example usage\n",
        "a = Tensor(-2.0, label='a')\n",
        "b = Tensor(5.0, label='b')\n",
        "c = Tensor(-4.0, label='c')\n",
        "\n",
        "ab = a + b; ab.label = 'a + b'\n",
        "f = ab * c; f.label = 'f'\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"output: {f}\")\n",
        "print(f\"df/da: {a.gradient}\")\n",
        "print(f\"df/db: {b.gradient}\")\n",
        "print(f\"df/dc: {c.gradient}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a: tensor=(-2.0)\n",
            "output: tensor=(-12.0)\n",
            "df/da: -4.0\n",
            "df/db: -4.0\n",
            "df/dc: 3.0\n"
          ]
        }
      ],
      "execution_count": 51
    },
    {
      "cell_type": "code",
      "id": "dc29c5a9851a9ca2",
      "metadata": {
        "id": "dc29c5a9851a9ca2"
      },
      "source": [
        "def trace(root):\n",
        "    # builds a set of all nodes and edges in a graph\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v._prev:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "    from graphviz import Digraph\n",
        "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "    nodes, edges = trace(root)\n",
        "    for n in nodes:\n",
        "        uid = str(id(n))\n",
        "        # for any value in the graph, create a rectangular ('record') node for it\n",
        "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.gradient), shape='record')\n",
        "        if n._operation:\n",
        "            # if this value is a result of some operation, create an op node for it\n",
        "            dot.node(name = uid + n._operation, label=n._operation)\n",
        "            # and connect this node to it\n",
        "            dot.edge(uid + n._operation, uid)\n",
        "    for n1, n2 in edges:\n",
        "        # connect n1 to the op node of n2\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._operation)\n",
        "    return dot"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "2e903d8c90f571a",
      "metadata": {
        "id": "2e903d8c90f571a"
      },
      "source": [
        "# Draw the graph\n",
        "draw_dot(f)\n",
        "\n",
        "# If there is a problem with displaying the graph, run the following command in the terminal (Linux only)\n",
        "# sudo apt-get install graphviz"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "bcb13aa09ef0f4af",
      "metadata": {
        "id": "bcb13aa09ef0f4af"
      },
      "source": [
        "Now let's move on to a simple neural network."
      ]
    },
    {
      "cell_type": "code",
      "id": "3f9ebf150e297445",
      "metadata": {
        "id": "3f9ebf150e297445"
      },
      "source": [
        "# Input values\n",
        "x1 = Tensor(2.0, label='x1')\n",
        "x2 = Tensor(3.0, label='x2')\n",
        "\n",
        "# Weights\n",
        "w1 = Tensor(-3.0, label='w1')\n",
        "w2 = Tensor(1.0, label='w2')\n",
        "\n",
        "# bias\n",
        "b = Tensor(6.0, label='b')\n",
        "\n",
        "# y = tanh(Wx + b) = tanh(w1*x1 + w2*x2 + b)\n",
        "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
        "y = x1w1x2w2 + b\n",
        "y = y.tanh(); y.label = 'y'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "7e543c53d8ec409e",
      "metadata": {
        "id": "7e543c53d8ec409e"
      },
      "source": [
        "# Calculate the gradient of y with respect to w1, w2, and b\n",
        "y.backward()\n",
        "print(f\"y: {y.data}\")\n",
        "print(f\"dy/dw1: {w1.gradient}\")\n",
        "print(f\"dy/dw2: {w2.gradient}\")\n",
        "print(f\"dy/db: {b.gradient}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "3046c24e1a94240a",
      "metadata": {
        "id": "3046c24e1a94240a"
      },
      "source": [
        "# Draw the graph\n",
        "draw_dot(y)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "34972521d62160a6",
      "metadata": {
        "id": "34972521d62160a6"
      },
      "source": [
        "### PyTorch Tensor\n",
        "\n",
        "PyTorch is a popular deep learning library that provides a tensor object similar to the one we've implemented.\n",
        "\n",
        "Let's implement the same example using PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "id": "254b7f18bff1fc44",
      "metadata": {
        "id": "254b7f18bff1fc44"
      },
      "source": [
        "import torch\n",
        "# Implement PyTorch tensor\n",
        "x1 = torch.tensor(2.0, requires_grad=False)\n",
        "x2 = torch.tensor(3.0, requires_grad=False)\n",
        "w1 = torch.tensor(-3.0, requires_grad=True)\n",
        "w2 = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(6.0, requires_grad=True)\n",
        "y = torch.tanh(w1 * x1 + w2 * x2 + b)\n",
        "\n",
        "# Calculate the gradient of y with respect to w1, w2, and b\n",
        "print(f\"y: {y.data.item()}\")\n",
        "y.backward()\n",
        "print(f\"dy/dw1: {w1.grad.item()}\")\n",
        "print(f\"dy/dw2: {w2.grad.item()}\")\n",
        "print(f\"dy/db: {b.grad.item()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c17e490aa4f1c213",
      "metadata": {
        "id": "c17e490aa4f1c213"
      },
      "source": [
        "## Part 2: Linear Regression\n",
        "\n",
        "Linear regression is a simple machine learning model that predicts the relationship between two variables.\n",
        "\n",
        "Let's implement a simple linear regression model using our custom tensor object. Then, we will train the model using the backpropagation algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63cd0afff9eb1fa5",
      "metadata": {
        "id": "63cd0afff9eb1fa5"
      },
      "source": [
        "### Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "e28c89d89c8d4e8",
      "metadata": {
        "id": "e28c89d89c8d4e8"
      },
      "source": [
        "# Linear network\n",
        "class Linear:\n",
        "    def __init__(self, input_size):\n",
        "        # y = Wx + b\n",
        "        self.weights = [Tensor(random.uniform(-1, 1)) for _ in range(input_size)]  # (input_size)\n",
        "        self.bias = Tensor(random.uniform(-1, 1))  # (1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        y = Wx + b\n",
        "        \"\"\"\n",
        "        x = sum((wi * xi for wi, xi in zip(self.weights, x)), self.bias)  # matrix multiplication by summing the products\n",
        "        # note: Dot product is not good for parallelization\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.weights + [self.bias]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "b74c7bf299837031",
      "metadata": {
        "id": "b74c7bf299837031"
      },
      "source": [
        "# Input values\n",
        "x = [Tensor(-2.0), Tensor(5.0), Tensor(-4.0), Tensor(1.0)]\n",
        "\n",
        "# Target value\n",
        "y = Tensor(-5.0)\n",
        "\n",
        "# Initialize the linear network\n",
        "model = Linear(input_size=4)\n",
        "\n",
        "# Example forward pass\n",
        "print(f\"Output: {model(x)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f10fd413ef00d81e",
      "metadata": {
        "id": "f10fd413ef00d81e"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training a neural network involves the following steps:\n",
        "1. Forward pass: Calculate the predicted value.\n",
        "2. Loss: Calculate the difference between the predicted value and the target value.\n",
        "3. Backward pass: Calculate the gradient of the loss with respect to the weights.\n",
        "4. Update weights: Update the weights using the gradients.\n",
        "\n",
        "Repeat the process until the loss is minimized.\n",
        "\n",
        "Let's implement a simple training loop for the linear regression model.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "33eb7fb9ff8fbdcb",
      "metadata": {
        "id": "33eb7fb9ff8fbdcb"
      },
      "source": [
        "lr = 0.01  # Learning rate\n",
        "\n",
        "# Training loop\n",
        "for step in range(10):\n",
        "    # Forward pass\n",
        "    logits = model(x)\n",
        "\n",
        "    # Loss\n",
        "    loss = (logits - y) ** 2  # MSE loss\n",
        "\n",
        "    # Backward pass\n",
        "    for param in model.parameters():\n",
        "        param.gradient = 0  # Zero the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    for param in model.parameters():\n",
        "        param.data = param.data - lr * param.gradient\n",
        "\n",
        "    print(f\"Step {step+1}, Loss: {loss.data}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}