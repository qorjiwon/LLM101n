{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32ac1a934eac9271",
      "metadata": {
        "id": "32ac1a934eac9271"
      },
      "source": [
        "# Lecture 1: Introduction to Machine Learning\n",
        "\n",
        "In this lecture, we will introduce the basics of machine learning.\n",
        "\n",
        "Let's start by exploring what neural network training looks like under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82b8f28897d2765a",
      "metadata": {
        "id": "82b8f28897d2765a"
      },
      "source": [
        "## Part 1: Backpropagation\n",
        "\n",
        "Backpropagation algorithm is the cornerstone of training neural networks. (used in all neural nets) It is a method to calculate the gradient of the loss function with respect to the weights of the network.\n",
        "\n",
        "**The main problem is... how do computers compute the gradient?**\n",
        "\n",
        "Humans get the gradient by calculating the derivative, then plug in the values.\n",
        "\n",
        "Computers can't do that:\n",
        "1. Derivatives are hard to calculate\n",
        "2. Neural networks are huge\n",
        "\n",
        "Let's build our own backpropagation algorithm from scratch step by step.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "afdaf122c0a24b14",
      "metadata": {
        "id": "afdaf122c0a24b14"
      },
      "source": [
        "# Importing libraries\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "id": "f8b173bdbbdff33b",
      "metadata": {
        "id": "f8b173bdbbdff33b"
      },
      "source": [
        "### Manual Gradient Calculation\n",
        "\n",
        "Let's manually calculate the gradient of a function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50519bf41896d5de",
      "metadata": {
        "id": "50519bf41896d5de"
      },
      "source": [
        "#### Example 1: Single Variable Function"
      ]
    },
    {
      "cell_type": "code",
      "id": "79bd41eabcac06c0",
      "metadata": {
        "id": "79bd41eabcac06c0"
      },
      "source": [
        "# Define a random function\n",
        "def f(x):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    # f(x) = 3x^2 - 4x + 5                                                         #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    out = 3*x*x - 4*x + 5\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "id": "5f29125addedc611",
      "metadata": {
        "id": "5f29125addedc611"
      },
      "source": [
        "f(2)  # f(2) = 3*2^2 - 4*2 + 5 = 9"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "34cf6c8c9460cedb",
      "metadata": {
        "id": "34cf6c8c9460cedb"
      },
      "source": [
        "# Plot the function\n",
        "xs = np.arange(-5, 5, 0.25)  # np.arrange(start, stop, step)\n",
        "ys = [f(x) for x in xs]\n",
        "plt.plot(xs, ys)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "8cc5601b2bfbdacf",
      "metadata": {
        "id": "8cc5601b2bfbdacf"
      },
      "source": [
        "What is the gradient of f(x) at x=2?\n",
        "- Human's solution:\n",
        "    - f'(x) = 6x - 4\n",
        "    - f'(2) = 6*2 - 4 = 8\n",
        "\n",
        "- Computer's solution:\n",
        "    - f'(2) = (f(2 + h) - f(2)) / h\n",
        "        - h: a small number (0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "id": "a4858fe8eb81441b",
      "metadata": {
        "id": "a4858fe8eb81441b"
      },
      "source": [
        "def gradient(function, x, _h=0.0001):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return grad"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "6c0972b63c72fe7c",
      "metadata": {
        "id": "6c0972b63c72fe7c"
      },
      "source": [
        "gradient(f, 2)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c9d3eeb8dd980e7",
      "metadata": {
        "id": "c9d3eeb8dd980e7"
      },
      "source": [
        "Let's calculate the gradient of a more complex function.\n",
        "\n",
        "![(x + y) * z](https://github.com/qorjiwon/LLM101n/blob/master/assets/(x+y)z.png?raw=1)\n",
        "\n",
        "(Image credit: Stanford CS231n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34ad426cfc324aeb",
      "metadata": {
        "id": "34ad426cfc324aeb"
      },
      "source": [
        "#### Example 2: Multi-Variable Function"
      ]
    },
    {
      "cell_type": "code",
      "id": "7c8986c366d8e99d",
      "metadata": {
        "id": "7c8986c366d8e99d"
      },
      "source": [
        "def f(x, y, z):\n",
        "    out = (x + y) * z\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "25857fe002e61146",
      "metadata": {
        "id": "25857fe002e61146"
      },
      "source": [
        "f(-2, 5, -4)  # f(-2, 5, -4) = (-2 + 5) * -4 = -12"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "a9171fd855b2b1b7",
      "metadata": {
        "id": "a9171fd855b2b1b7"
      },
      "source": [
        "Let's calculate the gradient of this function\n",
        "\n",
        "**Goal**:\n",
        "1. df/dx at x=-2\n",
        "2. df/dy at y=5\n",
        "3. df/dz at z=-4"
      ]
    },
    {
      "cell_type": "code",
      "id": "f2c53484de26a8ac",
      "metadata": {
        "id": "f2c53484de26a8ac"
      },
      "source": [
        "# Get the derivative of f(x, y, z) with respect to x, y, z\n",
        "h = 0.0001\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to x=2                                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dx: {df_dx}\")\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to y=-3                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dy: {df_dy}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to z=10                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dz: {df_dz}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "b4fecd506dd71a85",
      "metadata": {
        "id": "b4fecd506dd71a85"
      },
      "source": [
        "This time, let's calculate the gradient using the chain rule.\n",
        "\n",
        "\n",
        "![chain rule](https://github.com/qorjiwon/LLM101n/blob/master/assets/chain_rule.png?raw=1)\n",
        "\n",
        "(Image credit: Stanford CS231n)\n",
        "\n",
        "**Chain Rule**:\n",
        "\n",
        "q = x + y\n",
        "\n",
        "f = q * z\n",
        "\n",
        "- df/dx = df/dq * dq/dx\n",
        "- df/dy = df/dq * dq/dy"
      ]
    },
    {
      "cell_type": "code",
      "id": "26df18dea033aadf",
      "metadata": {
        "id": "26df18dea033aadf"
      },
      "source": [
        "# Get the derivative of f(x, y, z) with respect to x, y using the chain rule\n",
        "\n",
        "# Redefine the function f(x, y, z) and q(x, y)\n",
        "def f(q, z):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return out\n",
        "\n",
        "def q(x, y):\n",
        "    ################################################################################\n",
        "    # TODO:                                                                        #\n",
        "    ################################################################################\n",
        "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "    return out"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "cb02602405abfbf8",
      "metadata": {
        "id": "cb02602405abfbf8"
      },
      "source": [
        "# q\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to q=-2                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dq: {df_dq}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "3bf19d2256e21a37",
      "metadata": {
        "id": "3bf19d2256e21a37"
      },
      "source": [
        "# x\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of q with respect to x=-2                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"dq/dx: {dq_dx}\")\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to x=-2                                         #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dx: {df_dx}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "22816305e040df95",
      "metadata": {
        "id": "22816305e040df95"
      },
      "source": [
        "# y\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of q with respect to y=5                                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"dq/dy: {dq_dy}\")\n",
        "\n",
        "\n",
        "################################################################################\n",
        "# TODO:                                                                        #\n",
        "# Derivative of f with respect to y=5                                          #\n",
        "################################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "print(f\"df/dy: {df_dy}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "a1dd5a4407648515",
      "metadata": {
        "id": "a1dd5a4407648515"
      },
      "source": [
        "### Custom Auto-Grad Engine\n",
        "\n",
        "Neural networks are a series of functions that are composed together. Each function is a layer in the network. To get the gradient, we need to calculate the derivative of each function using the chain rule.\n",
        "\n",
        "**Let's make a custom tensor object that calculates and stores the gradient of the tensor.**\n",
        "1. Support basic operations: addition, multiplication, tanh, power\n",
        "2. Calculate and store the gradient of the tensor\n",
        "\n",
        "- Why not use numpy arrays?:\n",
        "    - We need to keep track of the operations and tensors that lead to this tensor in order to calculate the gradient.\n",
        "\n",
        "- Numerical vs Analytical\n",
        "    - Numerical differentiation: estimates the gradient using the finite difference approximation\n",
        "        -  f'(x) = (f(x + h) - f(x)) / h\n",
        "        - Slow\n",
        "        - Precision issues\n",
        "    - Analytical differentiation: derives the function symbolically using the chain rule\n",
        "        - Fast\n",
        "        - Precise\n",
        "        - Need to code the derivative of every operation"
      ]
    },
    {
      "cell_type": "code",
      "id": "b279b1065a769ef0",
      "metadata": {
        "id": "b279b1065a769ef0"
      },
      "source": [
        "# Example usage\n",
        "# a = Tensor(-2.0)\n",
        "# b = Tensor(5.0)\n",
        "# c = Tensor(-4.0)\n",
        "# f = (a + b) * c\n",
        "# f.backward()\n",
        "\n",
        "# print(f\"a: {a}\")               # a: tensor=(-2.0)\n",
        "# print(f\"output: {f}\")          # output: tensor=12.0\n",
        "# print(f\"df/da: {a.gradient}\")  # df/da: 1.0\n",
        "# print(f\"df/db: {b.gradient}\")  # df/db: 1.0\n",
        "# print(f\"df/dc: {c.gradient}\")  # df/dc: 3.0"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "71703911d4435375",
      "metadata": {
        "id": "71703911d4435375"
      },
      "source": [
        "#### Tensor version 1\n",
        "\n",
        "A simple tensor object that supports addition and multiplication."
      ]
    },
    {
      "cell_type": "code",
      "id": "b961d923e295de1f",
      "metadata": {
        "id": "b961d923e295de1f"
      },
      "source": [
        "class TensorV1:\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    # method to print the tensor\n",
        "    def __repr__(self):\n",
        "        return f\"tensor=({self.data})\"\n",
        "\n",
        "    # method to add two tensors\n",
        "    def __add__(self, other):  # self + other\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the addition operation.                                            #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output\n",
        "\n",
        "    # method to multiply two tensors\n",
        "    def __mul__(self, other):  # self * other\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the multiplication operation.                                      #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "79bced7cd35204a8",
      "metadata": {
        "id": "79bced7cd35204a8"
      },
      "source": [
        "# Example usage\n",
        "a = TensorV1(-2.0)\n",
        "b = TensorV1(5.0)\n",
        "c = TensorV1(-4.0)\n",
        "f = (a + b) * c\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"output: {f}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "37036ba643a2220c",
      "metadata": {
        "id": "37036ba643a2220c"
      },
      "source": [
        "#### Tensor version 2"
      ]
    },
    {
      "cell_type": "code",
      "id": "ff003f5d07d0e5dd",
      "metadata": {
        "id": "ff003f5d07d0e5dd"
      },
      "source": [
        "class TensorV2:\n",
        "    def __init__(self, data, _children=(), _operation=''):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)  # _children: tensors that lead to this tensor (ex: 2 * 3 = 6, 2 and 3 are children of 6)\n",
        "        self.gradient = 0\n",
        "        self._backward = lambda: None\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"tensor=({self.data})\"\n",
        "\n",
        "    def __add__(self, other):  # self + other\n",
        "        output = TensorV2(self.data + other.data, (self, other))\n",
        "        def _backward():\n",
        "            ################################################################################\n",
        "            # TODO:                                                                        #\n",
        "            # Implement the backward pass for addition.                                    #\n",
        "            # hint: use the chain rule (df/dx = df/dq * dq/dx, q = x + y)                  #\n",
        "            # df/dq -> output.gradient                                                     #\n",
        "            ################################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def __mul__(self, other):  # self * other\n",
        "        output = TensorV2(self.data * other.data, (self, other))\n",
        "        def _backward():\n",
        "            ################################################################################\n",
        "            # TODO:                                                                        #\n",
        "            # Implement the backward pass for multiplication.                              #\n",
        "            # hint: use the chain rule (df/dx = df/dq * dq/dx, q = x * y)                  #\n",
        "            # df/dq -> output.gradient                                                     #\n",
        "            ################################################################################\n",
        "            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    # method to calculate the gradient\n",
        "    # Goes through the graph in reverse topological order and calculate the gradient\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            node._backward()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "17b5a3f0347949",
      "metadata": {
        "id": "17b5a3f0347949"
      },
      "source": [
        "# Example usage\n",
        "a = TensorV2(-2.0)\n",
        "b = TensorV2(5.0)\n",
        "c = TensorV2(-4.0)\n",
        "f = (a + b) * c\n",
        "f.backward()\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"output: {f}\")\n",
        "print(f\"df/da: {a.gradient}\")\n",
        "print(f\"df/db: {b.gradient}\")\n",
        "print(f\"df/dc: {c.gradient}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "eb4b152ef95d1cd2",
      "metadata": {
        "id": "eb4b152ef95d1cd2"
      },
      "source": [
        "#### Tensor final version"
      ]
    },
    {
      "cell_type": "code",
      "id": "65624f2a4df6d632",
      "metadata": {
        "id": "65624f2a4df6d632"
      },
      "source": [
        "class Tensor:\n",
        "    def __init__(self, data, _children=(), _operation='', label=''):\n",
        "        self.data = data\n",
        "        self._prev = set(_children)\n",
        "        self.gradient = 0\n",
        "        self._backward = lambda: None\n",
        "        # for visualization\n",
        "        self._operation = _operation  # _operation: operation that lead to this tensor (ex: 2 * 3 = 6, * is the operation)\n",
        "        self.label = label  # label: name of the tensor\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"tensor=({self.data})\"\n",
        "\n",
        "    def __add__(self, other):  # self + other\n",
        "        output = Tensor(self.data + other.data, (self, other), '+')\n",
        "        def _backward():\n",
        "            self.gradient += 1 * output.gradient\n",
        "            other.gradient += 1 * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    def __mul__(self, other):  # self * other\n",
        "        output = Tensor(self.data * other.data, (self, other), '*')\n",
        "        def _backward():\n",
        "            self.gradient += other.data * output.gradient\n",
        "            other.gradient += self.data * output.gradient\n",
        "        output._backward = _backward\n",
        "        return output\n",
        "\n",
        "    # Activation function\n",
        "    def tanh(self):  # tanh(self)\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the tanh operation.                                                #\n",
        "        # hint: use math.tanh                                                          #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output\n",
        "\n",
        "    def __pow__(self, power):  # self ** power\n",
        "        assert isinstance(power, (int, float)), \"Power must be an int or a float\"\n",
        "        ################################################################################\n",
        "        # TODO:                                                                        #\n",
        "        # Implement the power operation.                                               #\n",
        "        ################################################################################\n",
        "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "        return output\n",
        "\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "        self.gradient = 1\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n",
        "\n",
        "    def __neg__(self): # -self\n",
        "        return self * Tensor(-1.0)\n",
        "\n",
        "    def __sub__(self, other): # self - other\n",
        "        return self + (-other)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "fac571f40884fec5",
      "metadata": {
        "id": "fac571f40884fec5"
      },
      "source": [
        "# Example usage\n",
        "a = Tensor(-2.0, label='a')\n",
        "b = Tensor(5.0, label='b')\n",
        "c = Tensor(-4.0, label='c')\n",
        "\n",
        "ab = a + b; ab.label = 'a + b'\n",
        "f = ab * c; f.label = 'f'\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print(f\"a: {a}\")\n",
        "print(f\"output: {f}\")\n",
        "print(f\"df/da: {a.gradient}\")\n",
        "print(f\"df/db: {b.gradient}\")\n",
        "print(f\"df/dc: {c.gradient}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "dc29c5a9851a9ca2",
      "metadata": {
        "id": "dc29c5a9851a9ca2"
      },
      "source": [
        "def trace(root):\n",
        "    # builds a set of all nodes and edges in a graph\n",
        "    nodes, edges = set(), set()\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.add(v)\n",
        "            for child in v._prev:\n",
        "                edges.add((child, v))\n",
        "                build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "def draw_dot(root):\n",
        "    from graphviz import Digraph\n",
        "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
        "    nodes, edges = trace(root)\n",
        "    for n in nodes:\n",
        "        uid = str(id(n))\n",
        "        # for any value in the graph, create a rectangular ('record') node for it\n",
        "        dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.gradient), shape='record')\n",
        "        if n._operation:\n",
        "            # if this value is a result of some operation, create an op node for it\n",
        "            dot.node(name = uid + n._operation, label=n._operation)\n",
        "            # and connect this node to it\n",
        "            dot.edge(uid + n._operation, uid)\n",
        "    for n1, n2 in edges:\n",
        "        # connect n1 to the op node of n2\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._operation)\n",
        "    return dot"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "2e903d8c90f571a",
      "metadata": {
        "id": "2e903d8c90f571a"
      },
      "source": [
        "# Draw the graph\n",
        "draw_dot(f)\n",
        "\n",
        "# If there is a problem with displaying the graph, run the following command in the terminal (Linux only)\n",
        "# sudo apt-get install graphviz"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "bcb13aa09ef0f4af",
      "metadata": {
        "id": "bcb13aa09ef0f4af"
      },
      "source": [
        "Now let's move on to a simple neural network."
      ]
    },
    {
      "cell_type": "code",
      "id": "3f9ebf150e297445",
      "metadata": {
        "id": "3f9ebf150e297445"
      },
      "source": [
        "# Input values\n",
        "x1 = Tensor(2.0, label='x1')\n",
        "x2 = Tensor(3.0, label='x2')\n",
        "\n",
        "# Weights\n",
        "w1 = Tensor(-3.0, label='w1')\n",
        "w2 = Tensor(1.0, label='w2')\n",
        "\n",
        "# bias\n",
        "b = Tensor(6.0, label='b')\n",
        "\n",
        "# y = tanh(Wx + b) = tanh(w1*x1 + w2*x2 + b)\n",
        "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
        "y = x1w1x2w2 + b\n",
        "y = y.tanh(); y.label = 'y'"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "7e543c53d8ec409e",
      "metadata": {
        "id": "7e543c53d8ec409e"
      },
      "source": [
        "# Calculate the gradient of y with respect to w1, w2, and b\n",
        "y.backward()\n",
        "print(f\"y: {y.data}\")\n",
        "print(f\"dy/dw1: {w1.gradient}\")\n",
        "print(f\"dy/dw2: {w2.gradient}\")\n",
        "print(f\"dy/db: {b.gradient}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "3046c24e1a94240a",
      "metadata": {
        "id": "3046c24e1a94240a"
      },
      "source": [
        "# Draw the graph\n",
        "draw_dot(y)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "34972521d62160a6",
      "metadata": {
        "id": "34972521d62160a6"
      },
      "source": [
        "### PyTorch Tensor\n",
        "\n",
        "PyTorch is a popular deep learning library that provides a tensor object similar to the one we've implemented.\n",
        "\n",
        "Let's implement the same example using PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "id": "254b7f18bff1fc44",
      "metadata": {
        "id": "254b7f18bff1fc44"
      },
      "source": [
        "import torch\n",
        "# Implement PyTorch tensor\n",
        "x1 = torch.tensor(2.0, requires_grad=False)\n",
        "x2 = torch.tensor(3.0, requires_grad=False)\n",
        "w1 = torch.tensor(-3.0, requires_grad=True)\n",
        "w2 = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(6.0, requires_grad=True)\n",
        "y = torch.tanh(w1 * x1 + w2 * x2 + b)\n",
        "\n",
        "# Calculate the gradient of y with respect to w1, w2, and b\n",
        "print(f\"y: {y.data.item()}\")\n",
        "y.backward()\n",
        "print(f\"dy/dw1: {w1.grad.item()}\")\n",
        "print(f\"dy/dw2: {w2.grad.item()}\")\n",
        "print(f\"dy/db: {b.grad.item()}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "c17e490aa4f1c213",
      "metadata": {
        "id": "c17e490aa4f1c213"
      },
      "source": [
        "## Part 2: Linear Regression\n",
        "\n",
        "Linear regression is a simple machine learning model that predicts the relationship between two variables.\n",
        "\n",
        "Let's implement a simple linear regression model using our custom tensor object. Then, we will train the model using the backpropagation algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63cd0afff9eb1fa5",
      "metadata": {
        "id": "63cd0afff9eb1fa5"
      },
      "source": [
        "### Model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "e28c89d89c8d4e8",
      "metadata": {
        "id": "e28c89d89c8d4e8"
      },
      "source": [
        "# Linear network\n",
        "class Linear:\n",
        "    def __init__(self, input_size):\n",
        "        # y = Wx + b\n",
        "        self.weights = [Tensor(random.uniform(-1, 1)) for _ in range(input_size)]  # (input_size)\n",
        "        self.bias = Tensor(random.uniform(-1, 1))  # (1)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "        y = Wx + b\n",
        "        \"\"\"\n",
        "        x = sum((wi * xi for wi, xi in zip(self.weights, x)), self.bias)  # matrix multiplication by summing the products\n",
        "        # note: Dot product is not good for parallelization\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.weights + [self.bias]"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "id": "b74c7bf299837031",
      "metadata": {
        "id": "b74c7bf299837031"
      },
      "source": [
        "# Input values\n",
        "x = [Tensor(-2.0), Tensor(5.0), Tensor(-4.0), Tensor(1.0)]\n",
        "\n",
        "# Target value\n",
        "y = Tensor(-5.0)\n",
        "\n",
        "# Initialize the linear network\n",
        "model = Linear(input_size=4)\n",
        "\n",
        "# Example forward pass\n",
        "print(f\"Output: {model(x)}\")"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "id": "f10fd413ef00d81e",
      "metadata": {
        "id": "f10fd413ef00d81e"
      },
      "source": [
        "### Training\n",
        "\n",
        "Training a neural network involves the following steps:\n",
        "1. Forward pass: Calculate the predicted value.\n",
        "2. Loss: Calculate the difference between the predicted value and the target value.\n",
        "3. Backward pass: Calculate the gradient of the loss with respect to the weights.\n",
        "4. Update weights: Update the weights using the gradients.\n",
        "\n",
        "Repeat the process until the loss is minimized.\n",
        "\n",
        "Let's implement a simple training loop for the linear regression model.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "33eb7fb9ff8fbdcb",
      "metadata": {
        "id": "33eb7fb9ff8fbdcb"
      },
      "source": [
        "lr = 0.01  # Learning rate\n",
        "\n",
        "# Training loop\n",
        "for step in range(10):\n",
        "    # Forward pass\n",
        "    logits = model(x)\n",
        "\n",
        "    # Loss\n",
        "    loss = (logits - y) ** 2  # MSE loss\n",
        "\n",
        "    # Backward pass\n",
        "    for param in model.parameters():\n",
        "        param.gradient = 0  # Zero the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    for param in model.parameters():\n",
        "        param.data = param.data - lr * param.gradient\n",
        "\n",
        "    print(f\"Step {step+1}, Loss: {loss.data}\")"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}