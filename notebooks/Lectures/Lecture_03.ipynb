{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Lecture 3: Multi-Layer Perceptrons\n",
    "\n",
    "In this lecture, we will introduce Multi-Layer Perceptrons (MLP).\n",
    "\n",
    "We will reproduce the following paper [A Neural Probabilistic Language Model](https://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)"
   ],
   "id": "284bed26807861ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "For a character-level language model, the input vector is a one-hot vector of size 27 (26 characters + 1 space).\n",
    "\n",
    "If the model is not a character-level language model, (e.g., word-level language model), the input vector size is the size of the vocabulary which is usually very large (e.g., 20,000).\n",
    "\n",
    "This leads to the curse of dimensionality.\n",
    "\n",
    "**Solution**: Use a lower-dimensional representation of the input vector.\n",
    "\n",
    "The hypothesis is that similar words will have similar representations (e.g., dog and cat). Let's find a way to embed words into a lower-dimensional space.\n",
    "\n",
    "**Example**\n",
    "- The cat is walking in the bedroom. (Train data)\n",
    "- A dog was running in a room. (Train data)\n",
    "- The cat was running in a room. (Train data)\n",
    "- A dog is walking in a bedroom. (Train data)\n",
    "- A cat was running in a <?> (Test data)\n",
    "\n",
    "The model should be able to predict the word \"room\"(or the similar words) in the test data."
   ],
   "id": "1df5ff9bcc4a9e24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MLP\n",
    "\n",
    "In the previous lecture, we have successfully implemented Bigram language model. \n",
    "In this lecture, we will implement a Multi-Layer Perceptron (MLP) language model.\n",
    "\n",
    "For practical reasons, let's use a character-level language model.\n",
    "\n",
    "![MLP](../../assets/mlp.png)"
   ],
   "id": "3091db96691073c9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Importing Libraries",
   "id": "8708acc13f08c1a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from utils import load_text, set_seed\n",
    "%matplotlib inline"
   ],
   "id": "37a60786dfb334e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configuration",
   "id": "a5ecfeadcb656ced"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@dataclass\n",
    "class MLPConfig:\n",
    "    root_dir: str = os.getcwd() + \"/../../\"\n",
    "    dataset_path: str = \"data/names.txt\"\n",
    "\n",
    "    # Tokenizer\n",
    "    vocab_size: int = 0  # Set later\n",
    "    \n",
    "    # Model\n",
    "    context_size: int = 3\n",
    "    d_embed: int = 2\n",
    "    d_hidden: int = 32\n",
    "    \n",
    "    # Training\n",
    "    batch_size: int = 32\n",
    "    lr: float = 2e-4\n",
    "    max_steps: int = 10000\n",
    "\n",
    "    seed: int = 101\n",
    "\n",
    "config = MLPConfig()"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Reproducibility",
   "id": "5e753e3f451b6b46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "set_seed(config.seed)\n",
    "generator = torch.Generator().manual_seed(config.seed)"
   ],
   "id": "95b8b6a2b1a7d33b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset",
   "id": "d949a0c1c1439c98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load text and split by lines\n",
    "names = load_text(config.root_dir + config.dataset_path).splitlines()"
   ],
   "id": "631634a0a4e5f9dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Tokenizer",
   "id": "d4e7c1b4ba601a75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "chars = [chr(i) for i in range(97, 123)]  # all alphabet characters\n",
    "chars.insert(0, \".\")  # Add special token\n",
    "config.vocab_size = len(chars)\n",
    "str2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2str = {idx: char for char, idx in str2idx.items()}"
   ],
   "id": "dd74c6d04eef45f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing\n",
    "\n",
    "We need to create a dataset of (Input, Target) pairs.\n",
    "- Input: current 3 characters\n",
    "- Output: next 1 character"
   ],
   "id": "3963012859d3b3ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prepare_dataset(_names):\n",
    "    _inputs, _targets = [], []\n",
    "\n",
    "    for name in _names:\n",
    "        #print(name)\n",
    "        context = [0] * config.context_size  # How many characters do we take to predict the next character\n",
    "        \n",
    "        for char in name + \".\":\n",
    "            idx = str2idx[char]\n",
    "            _inputs.append(context)\n",
    "            _targets.append(idx)\n",
    "            #print(''.join(idx2str[i] for i in context), '--->', idx2str[idx])\n",
    "            context = context[1:] + [idx]  # Shift the context by 1 character\n",
    "        \n",
    "        #print(\"=\"*10)\n",
    "\n",
    "    _inputs = torch.tensor(_inputs)\n",
    "    _targets = torch.tensor(_targets)\n",
    "    \n",
    "    return _inputs, _targets"
   ],
   "id": "a1fa2464c368e34a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "inputs, targets = prepare_dataset(names)\n",
    "\n",
    "print(f\"Number of Input, Target pairs: {len(inputs)}\")\n",
    "print(f\"Input shape: {inputs.shape}, Output shape: {targets.shape}\")\n",
    "print(f\"First (Input, Target): {inputs[0]}, {targets[0]}\")\n",
    "print(f\"Second (Input, Target): {inputs[1]}, {targets[1]}\")"
   ],
   "id": "469a5f98bb7fcbcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "The model consists of the following components:\n",
    "- Embedding\n",
    "- Hidden Layer\n",
    "- Output Layer\n"
   ],
   "id": "7262c9fc27cdc08a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Embedding\n",
    "\n",
    "Embedding is a lookup table that maps an input character to a lower-dimensional representation.\n",
    "\n",
    "Example\n",
    "- Input: 'a'\n",
    "- Output: [0.1, 0.2]\n",
    "\n",
    "[0.1, 0.2] is the represents the character 'a' in a lower-dimensional space."
   ],
   "id": "f5e39988bc299d7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embedding example\n",
    "C = torch.randn(27, 2)\n",
    "print(f\"Embedding shape: {C.shape}\")"
   ],
   "id": "85c27cc98404dd9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embedding example\n",
    "# a: [1, :]\n",
    "a_embed = C[1, :]\n",
    "print(f\"Embedding of 'a': {a_embed}\")"
   ],
   "id": "17c51b98f3982eac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "How **Forward Pass** works in MLP:\n",
    "1. Embed the input characters.\n",
    "2. Concatenate the embeddings.\n",
    "3. Pass the concatenated embeddings through a hidden layer.\n",
    "4. Pass the hidden layer output through an output layer.\n",
    "5. Get the output of shape (vocab_size=27).\n",
    "\n",
    "\n",
    "Don't know how to concatenate? PyTorch provides concatenation functionality. [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.cat.html)"
   ],
   "id": "2d2b658e69d919a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example forward pass\n",
    "# Input: \".em\"\n",
    "e_embed = C[0]    # .: (embedding_size=2)\n",
    "m_embed = C[5]    # e: (embedding_size=2)\n",
    "m_embed2 = C[13]  # m: (embedding_size=2)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Concatenate the embeddings                                                   #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f\"Concatenated shape: {x.shape}\")"
   ],
   "id": "6a9e6b3f7d5c8231",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Hidden Layer\n",
    "\n",
    "The hidden layer is a linear transformation followed by a non-linear activation function."
   ],
   "id": "f8177f096e7418b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hidden layer\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Implement the hidden layer                                                   #\n",
    "# Use tanh as your activation function.                                        #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f\"Hidden shape: {h.shape}\")"
   ],
   "id": "408d9c150ee5c42b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Output Layer\n",
    "\n",
    "To get the logits, we need to pass the hidden layer output through another linear transformation.\n",
    "\n",
    "The output layer is a linear transformation.\n",
    "\n",
    "Example\n",
    "- Input: any kind of vector\n",
    "- Output: a vector of size 27 (vocab_size) representing the probability of each character."
   ],
   "id": "f841bf9f574977e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Output layer\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Implement the output layer                                                   #\n",
    "# Activation function must be ???                                              #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Output: {y}\")\n",
    "print(f\"Sum of probabilities: {y.sum()}\")"
   ],
   "id": "6ef79a57f1c08f5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example of prediction\n",
    "print(f\"Input characters: {idx2str[0]}, {idx2str[5]}, {idx2str[13]}\")\n",
    "print(f\"Output character (probability): {idx2str[y.argmax().item()]}, {y.max()}\")"
   ],
   "id": "8ee0e192b1188559",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let's refactor the code\n",
    "\n",
    "- Input:\n",
    "    - Shape: (batch_size, context_size)\n",
    "    - Example: [[5, 12, 12], [12, 12, 5]]  # \"emm\", \"mme\"\n",
    "\n",
    "- Parameters:\n",
    "    - Embedding:\n",
    "        - Shape: (vocab_size, d_embed)\n",
    "    - W1:\n",
    "        - Shape: (d_embed * context_size, d_hidden)\n",
    "    - W2:\n",
    "        - Shape: (d_hidden, vocab_size)\n",
    "\n",
    "- Output:\n",
    "    - Shape: (batch_size, vocab_size)\n",
    "    - Example: [[0.04, 0.03, ..., 0.02], [0.02, 0.03, ..., 0.04]]\n",
    "\n",
    "What is a **mini-batch**? \n",
    "- It is a subset of the dataset.\n",
    "- In practice, the dataset is too large to fit into memory. Therefore, we divide the dataset into mini-batches, then feed the model batch by batch.\n",
    "- batch_size: the number of samples in a mini-batch"
   ],
   "id": "7e3ed6aef1236744"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Initialize the parameters                                                    #\n",
    "# C: (vocab_size, d_embed)                                                     #\n",
    "# W1: (d_embed * context_size, d_hidden)                                       #\n",
    "# b1: (d_hidden)                                                               #\n",
    "# W2: (d_hidden, vocab_size)                                                   #\n",
    "# b2: (vocab_size)                                                             #\n",
    "# Set requires_grad to True                                                    #\n",
    "################################################################################\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in parameters)}\")"
   ],
   "id": "fadace327323382",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "PyTorch provides a CrossEntropyLoss function. [PyTorch Documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "**Note**: Softmax is already included in the CrossEntropyLoss function.\n",
    "\n",
    "Change the learning rate so that the loss graph looks as following\n",
    "\n",
    "![Loss](../../assets/train_loss.png)"
   ],
   "id": "bbc57ec6f727432b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr = config.lr  # Change learning rate\n",
    "steps = []\n",
    "losses = []\n",
    "\n",
    "for i in range(config.max_steps):\n",
    "    # Mini-batch\n",
    "    idx = torch.randint(0, len(inputs), (config.batch_size,))\n",
    "    batch_input = inputs[idx]  # (batch_size, context_size)\n",
    "    batch_target = targets[idx]  # (batch_size)\n",
    "    if i == 0:\n",
    "        print(f\"Input batch shape: {batch_input.shape}\")\n",
    "        print(f\"Target batch shape: {batch_target.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    ################################################################################\n",
    "    # TODO:                                                                        #\n",
    "    # Implement the forward pass                                                   #\n",
    "    # 1. Embed the input characters.                                               #\n",
    "    # 2. Concatenate the embeddings.                                               #\n",
    "    # 3. Pass the concatenated embeddings through a hidden layer.                  #\n",
    "    # 4. Pass the hidden layer output through an output layer.                     #\n",
    "    # DO NOT INCLUDE SOFTMAX                                                       #\n",
    "    ################################################################################\n",
    "    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    # Embedding\n",
    "\n",
    "    # Concatenate\n",
    "\n",
    "    # Hidden layer\n",
    "\n",
    "    # Output layer\n",
    "\n",
    "    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    loss = F.cross_entropy(logits, batch_target)  # (batch_size)\n",
    "    \n",
    "    # Backward pass\n",
    "    for param in parameters:\n",
    "        param.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    for param in parameters:\n",
    "        param.data += -lr * param.grad\n",
    "    \n",
    "    # Track loss\n",
    "    steps.append(i)\n",
    "    losses.append(loss.log10().item())\n",
    "    \n",
    "# Plot loss\n",
    "plt.plot(steps, losses)\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ],
   "id": "e836179adeea5de1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Visualization of the embedding matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)  # dimensions of 0 and 1\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), idx2str[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')\n",
    "plt.show()"
   ],
   "id": "df72a0dca624d881",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Inference",
   "id": "dfcd744c7b998412"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def generate_name():\n",
    "    new_name = []\n",
    "    context = [0] * config.context_size\n",
    "    \n",
    "    while True:\n",
    "        ################################################################################\n",
    "        # TODO:                                                                        #\n",
    "        # 1. Forward pass                                                              #\n",
    "        # 2. Sample the next token                                                     #\n",
    "        # 3. Decode the token                                                          #\n",
    "        # 4. Update the start_idx                                                      #\n",
    "        # 5. Break if the next character is \".\"                                        #\n",
    "        # Hint: Use F.softmax to get the probabilities                                 #\n",
    "        ################################################################################\n",
    "        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        # Forward pass\n",
    "\n",
    "        # Sample\n",
    "\n",
    "        \n",
    "        # Update context\n",
    "\n",
    "        # Break if \".\"\n",
    "\n",
    "        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "        \n",
    "    return \"\".join(new_name)\n",
    "\n",
    "for _ in range(5):\n",
    "    print(generate_name())"
   ],
   "id": "abda449e10e08b5d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Change the learning rate to get better results",
   "id": "fee809d43627c44d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
